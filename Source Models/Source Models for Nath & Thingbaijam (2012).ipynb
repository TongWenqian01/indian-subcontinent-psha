{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source models for Nath & Thingbaijam (2012)\n",
    "\n",
    "Read the source description input files from the online supplementary material and write them to XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%autoreload 2\n",
    "import source_model_tools as smt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toolbox as tb\n",
    "import lxml.etree as et\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from StringIO import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "import hmtk.sources as src\n",
    "from openquake.hazardlib import tom, geo\n",
    "\n",
    "from hmtk.plotting.mapping import HMTKBaseMap\n",
    "from hmtk.parsers.source_model.nrml04_parser import nrmlSourceModelParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>zmin</th>\n",
       "      <th>zmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  zmin  zmax\n",
       "0   1     0    25\n",
       "1   2    25    70\n",
       "2   3    70   180\n",
       "3   4   180   300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ids = [1, 2, 3, 4]\n",
    "layer_depths_km = [0, 25, 70, 180, 300]\n",
    "\n",
    "df_layers = pd.DataFrame(zip(layer_ids, layer_depths_km[:-1], layer_depths_km[1:]), columns=['id','zmin','zmax'])\n",
    "df_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_recomputed = False\n",
    "\n",
    "# define some lists needed at different stages\n",
    "min_mags = [4.5, 5.5]\n",
    "\n",
    "# define the input file names from the original paper\n",
    "model_path = '../Data/nath2012probabilistic'\n",
    "if use_recomputed:\n",
    "    smoothed_model_path = '../Smoothed/Recomputed'\n",
    "else:\n",
    "    smoothed_model_path = model_path\n",
    "polygon_file_template = os.path.join(model_path,'polygonlay%d.txt')\n",
    "seismicity_file_template = os.path.join(model_path,'seismicitylay%d.txt')\n",
    "polygon_files = [polygon_file_template % i for i in layer_ids] \n",
    "seismicity_files = [seismicity_file_template % i for i in layer_ids] \n",
    "\n",
    "smoothed_data_template = os.path.join(smoothed_model_path,'lay%dsmooth%.1f.txt')\n",
    "smoothed_data_files = [[smoothed_data_template % (i, m) \n",
    "                        for i in layer_ids] for m in min_mags]\n",
    "\n",
    "# implement some column-name replacements\n",
    "variable_mapping = {'avalue': 'a', 'bvalue': 'b', 'stdbvalue': 'stdb'}\n",
    "\n",
    "# an input file supplies some auxiliary data\n",
    "aux_file = 'auxiliary data.csv'\n",
    "\n",
    "# define prefixes for the output file names and models\n",
    "areal_source_model_file = 'areal_source_model'\n",
    "smoothed_source_data_file = 'smoothed_source_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_polygons(file_name):\n",
    "    \"\"\"\n",
    "    Read polygon descriptions from text file into pandas.DataFrame. \n",
    "    \n",
    "    File format as per Nath & Thingbaijam (2012).\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        keys = f.readline().strip().split(',')\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            values = line.strip().split(',', len(keys) - 1)\n",
    "            entry = {}\n",
    "            for key, value in zip(keys, values):\n",
    "                key = key.strip('[]')\n",
    "                value = value.strip('[]').replace(';', '\\n')\n",
    "                if key == 'zoneid':\n",
    "                    value = int(value)\n",
    "                else:\n",
    "                    value = np.genfromtxt(StringIO(value), delimiter=',').tolist()\n",
    "                entry[key] = value\n",
    "            result.append(entry)\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read areal polygons and seismicity statistics for each layer\n",
    "areal_df = pd.DataFrame()\n",
    "areal_polygons_df = pd.DataFrame()\n",
    "for i, layer in df_layers.iterrows():\n",
    "        \n",
    "    # read seismicity and polygons\n",
    "    layer_seis_df = pd.read_csv(seismicity_files[i])\n",
    "    layer_seis_df.rename(columns=variable_mapping, inplace=True)\n",
    "    layer_poly_df = read_polygons(polygon_files[i])\n",
    "    \n",
    "    # fill in depths, specify source mechanisms, clean up dip & rake\n",
    "    n_zones = len(layer_seis_df)\n",
    "    idx = layer_seis_df.index\n",
    "    layer_seis_df['zmin'] = pd.Series(np.full(n_zones, layer['zmin']), index=idx)\n",
    "    layer_seis_df['zmax'] = pd.Series(np.full(n_zones, layer['zmax']), index=idx)\n",
    "    layer_seis_df['layerid'] = pd.Series(np.full(n_zones, layer['id']), index=idx)\n",
    "    layer_seis_df['rake'] = tb.wrap(layer_seis_df['rake'])\n",
    "    layer_seis_df['mechanism'] = pd.Series(\n",
    "        smt.focal_mechanisms(layer_seis_df['dip'], layer_seis_df['rake']), index=idx)\n",
    "    layer_seis_df.loc[layer_seis_df['dip'] == -1, 'dip'] = 45\n",
    "    layer_seis_df.loc[layer_seis_df['mechanism'] == 'undefined', 'rake'] = 90\n",
    "    layer_seis_df.loc[layer_seis_df['strike'] == -1, 'strike'] = 0\n",
    "    layer_seis_df['mmin'] = pd.Series(np.full(n_zones, min_mags[0]), index=idx) \n",
    "    \n",
    "    # put it all together\n",
    "    layer_source_df = pd.merge(layer_seis_df, layer_poly_df, on='zoneid')\n",
    "    areal_df = pd.concat([areal_df, layer_source_df], ignore_index=True)\n",
    "    areal_polygons_df = pd.concat([areal_polygons_df, layer_poly_df], ignore_index=True)\n",
    "\n",
    "# merge auxiliary data (crucially, tectonic zones)\n",
    "aux_df = pd.read_csv(aux_file)\n",
    "aux_df = aux_df.drop(['zmin','zmax','dip','rake','mechanism'], axis=1)\n",
    "areal_df = pd.merge(areal_df, aux_df, on='zoneid')\n",
    "\n",
    "# convert polygons coordinates to objects\n",
    "areal_df['polygon'] = [\n",
    "    smt.MyPolygon([geo.point.Point(lat, lon)\n",
    "                   for lat, lon in area_series['polygon coordinates']])\n",
    "    for _, area_series in areal_df.iterrows()]\n",
    "\n",
    "# convert zoneid to string - sort_and_reindex takes care of \"human\" sorting\n",
    "areal_df['zoneid'] = [str(item) for item in areal_df['zoneid']]\n",
    "areal_df = smt.sort_and_reindex(areal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    99.000000\n",
       "mean      0.095354\n",
       "std       0.024839\n",
       "min       0.030000\n",
       "25%       0.080000\n",
       "50%       0.100000\n",
       "75%       0.115000\n",
       "max       0.160000\n",
       "Name: stdb, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areal_df[areal_df['stdb'] != 0]['stdb'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux_df[aux_df['tectonic subregion'] == 'no seismicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show a summary including megathrust zones and bin statistics\n",
    "drop_columns = ['tectonic zone', 'region', 'concerns', 'zmax', 'zmin',\n",
    "                'polygon coordinates', 'polygon',\n",
    "                'aspect ratio', 'dip', 'rake', 'strike']\n",
    "display(pd.concat([areal_df.drop(drop_columns, axis=1).head(),\n",
    "                   areal_df.drop(drop_columns, axis=1).tail()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_df[areal_df['tectonic subregion'] == 'no seismicity'].drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "props = ['a','b','mmax']\n",
    "ranges = [np.arange(-0.5, 8, 1), np.arange(-0.1, 1.7, 0.2), np.arange(-0.5, 10, 1)]\n",
    "groups = areal_df.groupby('layerid')\n",
    "fig, axes = plt.subplots(nrows=len(props), ncols=1, figsize=(6, 3*len(props)))\n",
    "for prop, ax, bins in zip(props, axes, ranges):\n",
    "    data = [group[prop] for _, group in groups]\n",
    "    labels = ['layer %d' % id for id, _ in groups]\n",
    "    ax.hist(data, label=labels, stacked=True, bins=bins)\n",
    "    ax.set_ylabel(prop)\n",
    "axes[0].legend(loc='upper left');\n",
    "plt.savefig(\"ArealModelFmds.png\", dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print set(areal_df['tectonic subregion'])\n",
    "areal_df[areal_df['tectonic subregion'] == 'no seismicity'].drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read completeness table in anticipation of computing activity rates from smoothed event counts\n",
    "completeness_df = pd.read_csv('../Data/thingbaijam2011seismogenic/Table1.csv', header=[0,1], index_col=[0,1])\n",
    "#completeness_df.reset_index(inplace=True)\n",
    "completeness_df.columns = [' '.join(col).strip() for col in completeness_df.columns.values]\n",
    "completeness_df.reset_index(inplace=True)\n",
    "display(completeness_df)\n",
    "print completeness_df.columns\n",
    "print completeness_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read smoothed seismicity data\n",
    "smoothed_df_list = []\n",
    "for i, mag in enumerate(min_mags):   \n",
    "    layer_smoothed_df_list = []\n",
    "    for j, layer in pd.merge(completeness_df, df_layers).iterrows():\n",
    "        layer_smoothed_df = pd.read_csv(smoothed_data_files[i][j])\n",
    "        nu_mag = 'nu%s' % str(mag).replace('.','_')\n",
    "\n",
    "        rename_cols = {nu_mag: 'nu', 'lat':'latitude', 'lon':'longitude'}\n",
    "        layer_smoothed_df.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        layer_smoothed_df['layerid'] = layer['id']\n",
    "        layer_smoothed_df['mmin model'] = mag\n",
    "        layer_smoothed_df['mmin'] = mag\n",
    "        layer_smoothed_df['duration'] = (layer[str(mag) + ' end'] - \n",
    "                                         layer[str(mag) + ' start'] + 1)\n",
    "        if use_recomputed:\n",
    "            layer_smoothed_df['lambda'] = layer_smoothed_df['nu']\n",
    "            layer_smoothed_df['nu'] = layer_smoothed_df['lambda']*layer_smoothed_df['duration']\n",
    "        else:\n",
    "            layer_smoothed_df['lambda'] = layer_smoothed_df['nu']/layer_smoothed_df['duration']\n",
    "\n",
    "        layer_smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "    layer_smoothed_df = pd.concat(layer_smoothed_df_list)\n",
    "    smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "smoothed_df = pd.concat(smoothed_df_list)\n",
    "smoothed_df = smt.sort_and_reindex(smoothed_df)\n",
    "\n",
    "display(pd.concat((smoothed_df.head(), smoothed_df.tail())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop = 'lambda'\n",
    "fig, axes = plt.subplots(nrows=len(min_mags), ncols=1, figsize=(6, 3*len(min_mags)), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.1)\n",
    "for min_mag, ax in zip(min_mags, axes):\n",
    "    groups = smoothed_df[smoothed_df['mmin model'] == min_mag].groupby('layerid')\n",
    "    data = [np.log10(group[prop]).values for _, group in groups]\n",
    "    labels = ['layer %d' % id for id, _ in groups]\n",
    "    ax.hist(data, label=labels, stacked=True, bins=np.arange(-7,0,0.5))\n",
    "    ax.set_ylabel(('%s%g' % (prop, min_mag)).replace('.','_'))\n",
    "axes[0].legend(loc='upper left')\n",
    "plt.savefig(os.path.join(smoothed_model_path,'SmoothedModelActivityRates.png'), dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute all distances\n",
    "distances = np.full((len(smoothed_df), len(areal_df)), np.inf)\n",
    "for i, area_series in areal_df[areal_df['tectonic subregion'] != 'no seismicity'].iterrows():\n",
    "    at_depth = (smoothed_df['layerid'] == area_series['layerid']).values\n",
    "    mesh = geo.mesh.Mesh(\n",
    "        smoothed_df.loc[at_depth, 'longitude'].values,\n",
    "        smoothed_df.loc[at_depth, 'latitude'].values)\n",
    "    distances[at_depth, i] = area_series['polygon'].distances(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances.min(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zone_id = '118'\n",
    "this_model = smoothed_df['mmin model'] == 4.5\n",
    "this_zone = areal_df['zoneid'] == zone_id\n",
    "n_points_in_area = (distances == 0).sum(axis=0)\n",
    "drop_cols = ['tectonic subregion','region', 'tectonic zone',\n",
    "             'aspect ratio']\n",
    "fmd_cols = ['zoneid','layerid','a','b','stdb','mmax','stdmmax','mmin','polygon coordinates']\n",
    "\n",
    "print n_points_in_area[areal_df[this_zone].index[0]]\n",
    "display(areal_df[this_zone][fmd_cols])\n",
    "print areal_df[this_zone]['b']*areal_df[this_zone]['mmin']\n",
    "in_zone = (distances[:, areal_df[this_zone].index] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truncated Gutenberg-Richter magnitude-frequency distribution in OpenQuake implements\n",
    "$$\\lambda(m \\geq M) = 10^{a - b m} = e^{\\alpha - \\beta m}$$\n",
    "where, since $\\lambda$ is an annual rate, $10^a$ is too. If we ignore events below some threshold $m_{min}$ then the annual rate becomes\n",
    "$$\\lambda(m \\geq m_{min}) = e^{\\alpha - \\beta m_{min}} e^{-\\beta (m - m_{min})} = \\nu e^{-\\beta (m - m_{min})} $$\n",
    "Thus to compute the $a$ value required by OpenQuake from the activity rate $\\lambda$ for a given magnitude threshold, we must also take into account the $b$ value for the zone:\n",
    "$$a = \\log_{10}(\\lambda) + b m_{min}$$\n",
    "If instead what is provided is event counts $\\nu$ over some catalog duration $T$, then one simply computes $\\lambda = \\nu/T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each point in the smoothed model, choose the closest areal zone \n",
    "# and copy some useful columns over\n",
    "columns_to_copy = ['zoneid', 'zmax', 'zmin', 'tectonic subregion',\n",
    "                   'a', 'b', 'stdb', 'mmax', 'stdmmax',\n",
    "                   'rake', 'dip', 'strike', 'aspect ratio', 'msr']\n",
    "index_min = distances.argmin(axis=1)\n",
    "for i, area_series in areal_df[areal_df['tectonic subregion'] != 'no seismicity'].iterrows():\n",
    "    picked = i == index_min\n",
    "    for column in columns_to_copy:\n",
    "        smoothed_df.loc[picked, column] = area_series[column]\n",
    "    # grab mmax and bvalue from zone above if mmax zero for this zone\n",
    "    if area_series['mmax'] == 0:\n",
    "        alternate_zone = str(int(np.floor(float(area_series['zoneid'])/10)))\n",
    "        print 'For', area_series['zoneid'], 'using MFD from', alternate_zone\n",
    "        index_alt = np.where(areal_df['zoneid'] == alternate_zone)[0]\n",
    "        if len(index_alt) > 0:\n",
    "            smoothed_df.loc[picked, 'mmax'] = areal_df.loc[index_alt[0], 'mmax']\n",
    "            smoothed_df.loc[picked, 'b'] = areal_df.loc[index_alt[0], 'b']\n",
    "\n",
    "# computing the a-value for each point is now easy\n",
    "smoothed_df['nearest'] = distances.min(axis=1)\n",
    "smoothed_df['a'] = np.log10(smoothed_df['lambda']) + smoothed_df['b']*smoothed_df['mmin model']\n",
    "\n",
    "# for each area in the areal model: count the number of points and \n",
    "# sum the activity rates in the smoothed model. from the latter estimate\n",
    "# an equivalent a-value\n",
    "for mag in min_mags:\n",
    "    this_model = smoothed_df['mmin model'] == mag\n",
    "    sum_lambda = np.array([smoothed_df.loc[(distance == 0) & this_model.values, 'lambda'].sum()\n",
    "          for distance in distances.T])\n",
    "    areal_df['smoothed N ' +  str(mag)] = (distances[this_model.values, :] == 0).sum(axis=0)\n",
    "    areal_df['smoothed lambda ' +  str(mag)] = sum_lambda.round(4)\n",
    "    areal_df['smoothed a ' +  str(mag)] = (np.log10(sum_lambda) + areal_df['b']*mag).round(2)\n",
    "    areal_df['equiv a ' +  str(mag)] = areal_df['a']/areal_df['smoothed N ' +  str(mag)]\n",
    "    areal_df['areal lambda ' +  str(mag)] = (10**(areal_df['a'] - areal_df['b']*mag)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_drop = ['zmax', 'zmin', 'aspect ratio', 'msr',\n",
    "                'rake', 'dip', 'strike', 'stdb', 'stdmmax']\n",
    "no_mmax_df = smoothed_df[smoothed_df['mmax'] == 0]\n",
    "if len(no_mmax_df) > 0:\n",
    "    print \"Leftover points with no assigned mmax\"\n",
    "    display(no_mmax_df.drop(display_drop, axis=1).head())\n",
    "no_b_df = smoothed_df[smoothed_df['b'] == 0]\n",
    "if len(no_b_df) > 0:\n",
    "    print \"Leftover points with no assigned b\"\n",
    "    display(no_b_df.drop(display_drop, axis=1).head())\n",
    "no_zoneid_df = smoothed_df[smoothed_df['zoneid'].isnull()]\n",
    "if len(no_zoneid_df) > 0:\n",
    "    print \"Leftover points with no assigned zone id\"\n",
    "    display(no_zoneid_df.drop(display_drop, axis=1).head())\n",
    "if not len(no_mmax_df) and not len(no_b_df) and not len(no_zoneid_df):\n",
    "    print \"SUCCESS: No points with unassigned MFD or zone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((smoothed_df.head(), smoothed_df.tail())).drop(display_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_lon_lat = (smoothed_df['longitude'] == 98.0) & (smoothed_df['latitude'] == 3.8)\n",
    "display(smoothed_df[this_lon_lat].drop(display_drop, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_display_drop = display_drop + ['polygon','polygon coordinates','tectonic zone','concerns','layerid']\n",
    "pd.concat((areal_df.head(), areal_df.tail())).drop(areal_display_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity_df = areal_df[[\n",
    "        'zoneid', 'layerid', 'areal lambda 4.5', 'areal lambda 5.5', \n",
    "        'smoothed lambda 4.5', 'smoothed lambda 5.5', 'smoothed N 4.5', 'smoothed N 5.5']]\n",
    "activity_df = activity_df.rename(\n",
    "    columns={'areal lambda 4.5': 'areal 4.5', 'smoothed lambda 4.5': 'smoothed 4.5',\n",
    "             'areal lambda 5.5': 'areal 5.5', 'smoothed lambda 5.5': 'smoothed 5.5',\n",
    "             'smoothed N 4.5': 'N 4.5', 'smoothed N 5.5': 'N 5.5'})\n",
    "\n",
    "for layer_id in layer_ids:\n",
    "    series = pd.Series(activity_df[activity_df['layerid'] == layer_id].sum(axis=0))\n",
    "    series['layerid'] = layer_id\n",
    "    series['zoneid'] = 'All'\n",
    "    activity_df = activity_df.append(series, ignore_index=True)\n",
    "\n",
    "series = pd.Series(activity_df[activity_df['zoneid'] == 'All'].sum(axis=0))\n",
    "series['layerid'] = 'All'\n",
    "series['zoneid'] = 'All'\n",
    "activity_df = activity_df.append(series, ignore_index=True)\n",
    "activity_df['ratio 4.5'] = (activity_df['smoothed 4.5']/activity_df['areal 4.5']).round(2)\n",
    "activity_df['ratio 5.5'] = (activity_df['smoothed 5.5']/activity_df['areal 5.5']).round(2)\n",
    "\n",
    "picked_zones = [(activity_df['zoneid'] == zone).argmax() for zone in ['21', '22']]\n",
    "display(pd.concat((activity_df.loc[picked_zones, :], activity_df[activity_df['zoneid'] == 'All'])))\n",
    "activity_df.to_csv(os.path.join(smoothed_model_path,'activity_rates_by_zone_areal_vs_smoothed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try computing layers directly\n",
    "layer_activity_df = pd.DataFrame()\n",
    "layer_activity_df.index.name = 'layerid'\n",
    "\n",
    "for layer_id in layer_ids:\n",
    "    in_areal_layer = areal_df['layerid'] == layer_id\n",
    "    in_smoothed_layer = smoothed_df['layerid'] == layer_id\n",
    "    in_a_zone = smoothed_df['nearest'] == 0\n",
    "    \n",
    "    layer_series = pd.Series()\n",
    "    for mag in min_mags:\n",
    "\n",
    "        this_model = smoothed_df['mmin model'] == mag\n",
    "        layer_series = layer_series.append(pd.Series({\n",
    "            'areal ' + str(mag): (10**(areal_df[in_areal_layer]['a'] - \n",
    "                                       areal_df[in_areal_layer]['b']*mag)).sum().round(1),\n",
    "            'smoothed ' + str(mag): smoothed_df[in_smoothed_layer & this_model & in_a_zone]['lambda'].sum().round(1), \n",
    "            }, name=layer_id))\n",
    "    layer_activity_df = layer_activity_df.append(layer_series)\n",
    "\n",
    "layer_activity_df = layer_activity_df.append(pd.Series(layer_activity_df.sum(axis=0), name='Total'))\n",
    "for mag in min_mags:\n",
    "    layer_activity_df['ratio ' + str(mag)] = (layer_activity_df['smoothed ' + str(mag)]/\n",
    "                                              layer_activity_df['areal ' + str(mag)]).round(2)\n",
    "\n",
    "display(layer_activity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catalogue_df = pd.read_csv('../Catalogue/SACAT1900_2008v2.txt', sep='\\t')\n",
    "catalogue_df['SHOCK_TYPE'].value_counts().plot(kind='pie', figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign zones to events\n",
    "catalogue_df['zoneid'] = -1\n",
    "for i, area_series in areal_df.iterrows():\n",
    "    at_depth = ((catalogue_df['DEPTH'] >= area_series['zmin']) & \n",
    "                (catalogue_df['DEPTH'] < area_series['zmax']))\n",
    "    mesh = geo.mesh.Mesh(catalogue_df['LON'].values,\n",
    "                         catalogue_df['LAT'].values)\n",
    "    in_area = area_series['polygon'].distances(mesh) == 0\n",
    "    catalogue_df.loc[at_depth & in_area, 'zoneid'] = area_series['zoneid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((catalogue_df.head(), catalogue_df.tail()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each minimum magnitude and layer work out the activity rates\n",
    "catalogue_activity_df = pd.DataFrame()\n",
    "for _, layer in pd.merge(completeness_df, df_layers).iterrows():\n",
    "    layer_results = pd.Series()\n",
    "    for mag in reversed(min_mags):\n",
    "        above_thresh = catalogue_df['MAG_MW'] >= mag\n",
    "        start = layer[str(mag) + ' start']\n",
    "        end = layer[str(mag) + ' end']\n",
    "        at_depth = ((catalogue_df['DEPTH'] >= layer['zmin']) & \n",
    "                    (catalogue_df['DEPTH'] < layer['zmax']))\n",
    "        in_years = ((catalogue_df['YEAR'] >= start) & \n",
    "                    (catalogue_df['YEAR'] <= end))\n",
    "        in_a_zone = catalogue_df['zoneid'] != -1\n",
    "        is_mainshock = catalogue_df['SHOCK_TYPE'] == 'Mainshock'\n",
    "        subcat_df = catalogue_df[above_thresh & at_depth & in_years & in_a_zone & is_mainshock]\n",
    "        layer_results = layer_results.append(pd.Series({\n",
    "                'catalogue ' + str(mag): round(float(len(subcat_df))/(end - start + 1), 1), \n",
    "            }, name=layer['id']))\n",
    "    catalogue_activity_df = catalogue_activity_df.append(layer_results)\n",
    "catalogue_activity_df = catalogue_activity_df.append(pd.Series(catalogue_activity_df.sum(axis=0), name='Total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity_df = layer_activity_df.join(catalogue_activity_df)\n",
    "model_type = [item.split()[0] for item in activity_df.columns]\n",
    "model_type = ['catalogue' if item == 'cat' else item for item in model_type]\n",
    "min_mag = [float(item.split()[1]) for item in activity_df.columns]\n",
    "activity_df.columns = [min_mag, model_type]\n",
    "multi_cols = pd.MultiIndex.from_tuples(\n",
    "    [(4.5, 'areal'), (4.5, 'smoothed'), (4.5, 'catalogue'), \n",
    "     (5.5, 'areal'), (5.5, 'smoothed'), (5.5, 'catalogue'),], \n",
    "    names=['source', 'minimum magnitude'])\n",
    "#activity_df[('layerid', 'minimum magnitude')] = activity_df.index\n",
    "#activity_df.index = range(len(activity_df))\n",
    "\n",
    "activity_df = activity_df[multi_cols]\n",
    "display(activity_df)\n",
    "summary_tex = os.path.join(\n",
    "    smoothed_model_path,\n",
    "    'activity_rates_by_layer_areal_vs_smoothed_vs_catalogue.tex')\n",
    "activity_df.to_latex(summary_tex, index_names=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_mag = 5.5\n",
    "layer_id = 1\n",
    "model_layer = (smoothed_df['mmin model'] == min_mag) & \\\n",
    "    (smoothed_df['layerid'] == layer_id)\n",
    "subset_df = smoothed_df[model_layer].copy()\n",
    "\n",
    "lon_list = sorted(list(set(subset_df['longitude'])))\n",
    "lat_list = sorted(list(set(subset_df['latitude'])))\n",
    "\n",
    "lon_min, lon_max = [min(lon_list), max(lon_list)]\n",
    "lat_min, lat_max = [min(lat_list), max(lat_list)]\n",
    "lon_res = np.diff(lon_list).min().round(2)\n",
    "lat_res = np.diff(lat_list).min().round(2)\n",
    "\n",
    "lon_list = np.arange(lon_min, lon_max + lon_res, lon_res).round(2)\n",
    "lat_list = np.arange(lat_min, lat_max + lat_res, lat_res).round(2)\n",
    "lat_grid, lon_grid = np.meshgrid(lat_list, lon_list)\n",
    "\n",
    "# assign known values\n",
    "num_columns = ['a', 'b', 'nu', 'lambda']\n",
    "txt_columns = ['tectonic subregion']\n",
    "data = {}\n",
    "for column in num_columns:\n",
    "    data[column] = np.full_like(lon_grid, np.nan)\n",
    "for column in txt_columns:\n",
    "    data[column] = np.full_like(lon_grid, '', dtype='object')\n",
    "\n",
    "for _, point_series in subset_df.iterrows():\n",
    "    i = int(round((point_series['longitude'] - lon_min)/lon_res))\n",
    "    j = int(round((point_series['latitude'] - lat_min)/lat_res))\n",
    "    for column in num_columns:\n",
    "        data[column][i, j] = point_series[column]\n",
    "    for column in txt_columns:\n",
    "        data[column][i, j] = point_series[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = 'a'\n",
    "if param in ['nu', 'lambda']:\n",
    "    log_scale = True\n",
    "else:\n",
    "    log_scale = False\n",
    "limits = (np.nanmin(data[param]), np.nanmax(data[param]))\n",
    "plt.hist(data[param].ravel(), range=limits, log=log_scale);\n",
    "plt.savefig(\"SmoothedEquivalentMap_%s_mmin%g_layer%d.png\" % (param, min_mag, layer_id), dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca()\n",
    "if log_scale:\n",
    "    plt.imshow(data[param], cmap='jet', origin='lower', aspect='equal', \n",
    "               extent=(lon_min, lon_max, lat_min, lat_max), \n",
    "               norm=LogNorm(vmin=limits[0], vmax=limits[1]))\n",
    "else:\n",
    "    plt.imshow(data[param], cmap='jet', origin='lower', aspect='equal', \n",
    "               extent=(lon_min, lon_max, lat_min, lat_max))\n",
    "\n",
    "plt.colorbar(shrink=0.5)\n",
    "ax.set_xlabel(u'Longitude (°E)')\n",
    "ax.set_ylabel(u'Latitude (°N)')\n",
    "ax.grid()\n",
    "plt.savefig(os.path.join(smoothed_model_path,\n",
    "                         'SmoothedEquivalentMap_%s_mmin%g_layer%d.png' % (param, min_mag, layer_id)), \n",
    "            dpi=300, transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where([item in ['118','119','912','906','928','936'] for item in areal_df['zoneid']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write areal model data to TSV file ==> when this completes, logic trees can be generated\n",
    "areal_output_df = smt.sort_and_reindex(smt.add_name_id(\n",
    "    smt.twin_source_by_magnitude(areal_df)).drop(['polygon'], axis=1))\n",
    "areal_output_df.to_csv(areal_source_model_file + '.tsv', sep='\\t', index=False)\n",
    "areal_output_df = smt.sort_and_reindex(smt.add_name_id(\n",
    "    areal_df).drop(['polygon'], axis=1))\n",
    "areal_output_df.to_csv(areal_source_model_file + '_no_twin.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write each layer of areal source model to KML with added binwise rates\n",
    "areal_kml_df = smt.add_name_id(smt.add_binwise_rates(areal_df).drop(['polygon'], axis=1))\n",
    "for layer_id in layer_ids:\n",
    "    this_layer = areal_kml_df['layerid'] == layer_id\n",
    "    temp_df = areal_kml_df.drop(['layerid'], axis=1)\n",
    "    smt.source_df_to_kml(temp_df.loc[this_layer, :], \n",
    "        '%s layer %d' % (areal_source_model_file, layer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write areal source model with megathrust sources twinned to NRML\n",
    "areal_source_list = smt.source_df_to_list(\n",
    "    smt.add_name_id(smt.twin_source_by_magnitude(areal_df)))\n",
    "areal_source_model = src.source_model.mtkSourceModel(\n",
    "    identifier='1', \n",
    "    name=os.path.split(model_path)[1] + ' areal', \n",
    "    sources=areal_source_list)\n",
    "areal_source_model.serialise_to_nrml(areal_source_model_file + '.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write point source models without twinning to TSV file\n",
    "for min_mag in min_mags:\n",
    "    smoothed_output_df = smoothed_df[smoothed_df['mmin model'] == min_mag]\n",
    "    smoothed_output_df = smt.sort_and_reindex(smt.add_name_id(smoothed_output_df))\n",
    "    smoothed_output_df.drop(['mmin model'], axis=1, inplace=True)\n",
    "    smoothed_output_df['lambda'] = tb.limit_precision(smoothed_output_df['lambda'].values)\n",
    "    smoothed_output_df['a'] = tb.limit_precision(smoothed_output_df['a'].values)\n",
    "    smoothed_tsv= '%s_%s_mmin_%g.tsv' % (\n",
    "        os.path.split(smoothed_model_path)[1], smoothed_source_data_file, min_mag)\n",
    "    smoothed_output_df.to_csv(smoothed_tsv, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thin smoothed DataFrame so that resulting files are small enough to be read into QGIS\n",
    "smoothed_kml_df = smoothed_df.copy()\n",
    "drop_columns = ['aspect ratio','rake','dip','strike','msr','stdb','stdmmax']\n",
    "smoothed_kml_df.drop(drop_columns, axis=1, inplace=True)\n",
    "res_deg = 0.2\n",
    "this_lon_lat = ((((smoothed_kml_df['latitude'] % res_deg).round(2) % res_deg) == 0) &\n",
    "                (((smoothed_kml_df['longitude'] % res_deg).round(2) % res_deg) == 0))\n",
    "smoothed_kml_df = smoothed_kml_df[this_lon_lat]\n",
    "resolution_adjustment = float(len(smoothed_df))/float(len(smoothed_kml_df))\n",
    "print 'Adjusting activity for a change in resolution of %g' % resolution_adjustment\n",
    "smoothed_kml_df['nu'] = smoothed_kml_df['nu']*resolution_adjustment\n",
    "smoothed_kml_df['lambda'] = smoothed_kml_df['lambda']*resolution_adjustment\n",
    "smoothed_kml_df['a'] = smoothed_kml_df['a'] + np.log10(resolution_adjustment)\n",
    "\n",
    "# write each layer to CSV with added binwise rates\n",
    "for min_mag in reversed(min_mags):\n",
    "    for layer_id in reversed(layer_ids):\n",
    "        temp_df = smt.add_name_id(smoothed_kml_df)\n",
    "        this_model_layer = ((temp_df['mmin model'] == min_mag) &\n",
    "                   (temp_df['layerid'] == layer_id))\n",
    "        temp_df.drop(['zmax', 'zmin', 'mmin model', 'layerid'], axis=1, inplace=True)\n",
    "        model_name = '%s %s mmin %g layer %d' % (os.path.split(smoothed_model_path)[1],\n",
    "                                              smoothed_source_data_file, min_mag, layer_id) \n",
    "        \n",
    "        # smt.source_df_to_kml(temp_df.loc[this_model_layer, :], model_name)\n",
    "        \n",
    "        csv_file = model_name.replace(' ', '_') + '.csv'\n",
    "        temp_df.loc[this_model_layer, :].to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write point source models with megathrust sources twinned to NRML\n",
    "for min_mag in min_mags:\n",
    "    this_model = smoothed_df['mmin model'] == min_mag \n",
    "    smoothed_source_list = smt.source_df_to_list(\n",
    "        smt.add_name_id(smt.twin_source_by_magnitude(smoothed_df.loc[this_model, :])))\n",
    "    smoothed_source_model = src.source_model.mtkSourceModel(\n",
    "        identifier='1', \n",
    "        name=os.path.split(model_path)[1] + 'smoothed m_min=%g' % min_mag, \n",
    "        sources=smoothed_source_list)\n",
    "    file_name = '%s_%s_mmin%g.xml' % (os.path.split(smoothed_model_path)[1],\n",
    "                                      smoothed_source_data_file, min_mag)\n",
    "    smoothed_source_model.serialise_to_nrml(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PseudoCatalogue:\n",
    "    \"\"\"\n",
    "    ugly hack for plotting source mechanisms: \n",
    "    construct pseudo-cataloge from pandas.DataFrame\n",
    "    \"\"\"\n",
    "    def __init__(self, source_model, select_depth='all'):\n",
    "        data = []\n",
    "        oq_sources = source_model.convert_to_oqhazardlib(tom.PoissonTOM(1.0))\n",
    "        for source in oq_sources:\n",
    "            \n",
    "            longitude = np.mean(source.polygon.lons)\n",
    "            latitude = np.mean(source.polygon.lats)\n",
    "            strike = source.nodal_plane_distribution.data[0][1].strike\n",
    "            dip = source.nodal_plane_distribution.data[0][1].dip\n",
    "            rake = source.nodal_plane_distribution.data[0][1].rake\n",
    "            magnitude = source.get_min_max_mag()[1]*2.5\n",
    "            depth = source.hypocenter_distribution.data[0][1]\n",
    "            name = source.id\n",
    "            \n",
    "            if select_depth == 'all' or depth == select_depth:\n",
    "                data.append({'longitude': longitude, 'latitude': latitude,\n",
    "                             'strike1': strike, 'dip1': dip, 'rake1': rake,\n",
    "                             'magnitude': magnitude, 'depth': depth, 'id': name})\n",
    "        self.data = pd.DataFrame(data)\n",
    "\n",
    "    def get_number_tensors(self):\n",
    "        return len(self.data.magnitude)\n",
    "\n",
    "catalogue = PseudoCatalogue(areal_source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_config = {\"min_lon\": 60, \"max_lon\": 105, \n",
    "              \"min_lat\": 0,  \"max_lat\": 40, \"resolution\": \"l\",\n",
    "              \"parallel_meridian_spacing\": 10}\n",
    "parser = nrmlSourceModelParser(areal_source_model_file + '.xml')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "for depth, ax in zip(sorted(list(set(catalogue.data['depth']))), axes.ravel()):\n",
    "    fig.sca(ax)\n",
    "    tb.annotate('%g km' % depth, 'lower left')\n",
    "    basemap = HMTKBaseMap(map_config, None)\n",
    "\n",
    "    source_model_read = parser.read_file('Areal Source Model')\n",
    "    selected_sources = [source for source in source_model_read.sources \n",
    "                    if source.hypo_depth_dist.data[0][1] == depth]\n",
    "    source_model_read.sources = selected_sources    \n",
    "    selected_catalogue = PseudoCatalogue(source_model_read)\n",
    "\n",
    "    basemap.add_source_model(source_model_read, overlay=True) \n",
    "    basemap.add_focal_mechanism(selected_catalogue, magnitude=False)\n",
    "    for _, item in selected_catalogue.data.iterrows():\n",
    "        plt.annotate(s=item.id, xy=(item.longitude, item.latitude))\n",
    "        \n",
    "plt.savefig(\"ArealModelFocalMechanisms.pdf\", transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
