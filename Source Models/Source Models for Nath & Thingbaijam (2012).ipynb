{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source models for Nath & Thingbaijam (2012)\n",
    "\n",
    "Read the source description input files from the online supplementary material and write them to XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%autoreload 2\n",
    "import source_model_tools as smt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toolbox as tb\n",
    "import lxml.etree as et\n",
    "import matplotlib.pyplot as plt\n",
    "from StringIO import StringIO\n",
    "\n",
    "import hmtk.sources as src\n",
    "from openquake.hazardlib import tom\n",
    "\n",
    "from hmtk.plotting.mapping import HMTKBaseMap\n",
    "from hmtk.parsers.source_model.nrml04_parser import nrmlSourceModelParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_ids = [1, 2, 3, 4]\n",
    "layer_depths_km = [0, 25, 70, 180, 300]\n",
    "min_mags = [4.5, 5.5]\n",
    "\n",
    "model_path = 'nath2012probabilistic'\n",
    "polygon_file_template = os.path.join(model_path,'polygonlay%d.txt')\n",
    "seismicity_file_template = os.path.join(model_path,'seismicitylay%d.txt')\n",
    "aux_file = 'auxiliary data.csv'\n",
    "smoothed_data_template = os.path.join(model_path,'lay%dsmooth%.1f.txt')\n",
    "areal_source_model = 'areal_source_model.xml'\n",
    "smoothed_model_template = 'smoothed_source_mmin%.1f_model.xml'\n",
    "all_source_data_file = areal_source_model.replace('.xml','.tsv')\n",
    "\n",
    "polygon_files = [polygon_file_template % i for i in layer_ids] \n",
    "seismicity_files = [seismicity_file_template % i for i in layer_ids] \n",
    "smoothed_files = [[smoothed_data_template % (i, m) for i in layer_ids] \n",
    "                  for m in min_mags]\n",
    "smoothed_model_files = [smoothed_model_template % m for m in min_mags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_polygons(file_name):\n",
    "    \"\"\"\n",
    "    Read polygon descriptions from text file into pandas.DataFrame. \n",
    "    \n",
    "    File format as per Nath & Thingbaijam (2012).\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        keys = f.readline().strip().split(',')\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            values = line.strip().split(',', len(keys) - 1)\n",
    "            entry = {}\n",
    "            for key, value in zip(keys, values):\n",
    "                key = key.strip('[]')\n",
    "                value = value.strip('[]').replace(';', '\\n')\n",
    "                if key == 'zoneid':\n",
    "                    value = int(value)\n",
    "                else:\n",
    "                    value = np.genfromtxt(StringIO(value), delimiter=',')\n",
    "                entry[key] = value\n",
    "            result.append(entry)\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_source_data = pd.DataFrame()\n",
    "all_polygons = pd.DataFrame()\n",
    "loop_data = zip(layer_ids, seismicity_files, polygon_files, \n",
    "                layer_depths_km[:-1], layer_depths_km[1:])\n",
    "for i, seismicity_file, polygon_file, z_min, z_max in loop_data:\n",
    "        \n",
    "    # read seismicity and polygons\n",
    "    layer_seismicity = pd.read_csv(seismicity_file)\n",
    "    layer_polygons = read_polygons(polygon_file)\n",
    "    \n",
    "    # fill in depths, specify source mechanisms, clean up dip & rake\n",
    "    n_zones = len(layer_seismicity)\n",
    "    idx = layer_seismicity.index\n",
    "    layer_seismicity['zmin'] = pd.Series(np.full(n_zones, z_min), index=idx)\n",
    "    layer_seismicity['zmax'] = pd.Series(np.full(n_zones, z_max), index=idx)\n",
    "    layer_seismicity['layerid'] = pd.Series(np.full(n_zones, i), index=idx)\n",
    "    layer_seismicity['rake'] = tb.wrap(layer_seismicity['rake'])\n",
    "    layer_seismicity['mechanism'] = pd.Series(\n",
    "        smt.focal_mechanisms(layer_seismicity['dip'], layer_seismicity['rake']), index=idx)\n",
    "    layer_seismicity.loc[layer_seismicity['dip'] == -1, 'dip'] = 45\n",
    "    layer_seismicity.loc[layer_seismicity['mechanism'] == 'undefined', 'rake'] = 90\n",
    "    layer_seismicity.loc[layer_seismicity['strike'] == -1, 'strike'] = 0\n",
    "    layer_seismicity['mmin'] = pd.Series(np.full(n_zones, min_mags[0]), index=idx) \n",
    "    \n",
    "    # put it all together\n",
    "    layer_source_data = pd.merge(layer_seismicity, layer_polygons, on='zoneid')\n",
    "    all_source_data = pd.concat([all_source_data, layer_source_data], ignore_index=True)\n",
    "    all_polygons = pd.concat([all_polygons, layer_polygons], ignore_index=True)\n",
    "\n",
    "aux_data = pd.read_csv(aux_file)\n",
    "aux_data = aux_data.drop(['zmin','zmax','dip','rake','mechanism'], axis=1)\n",
    "all_source_data = pd.merge(all_source_data, aux_data, on='zoneid')\n",
    "\n",
    "all_source_data = all_source_data.set_index(all_source_data['zoneid'])\n",
    "\n",
    "# write everything to kml\n",
    "for i in set(all_source_data['layerid']):\n",
    "    indices = all_source_data['layerid'] == i\n",
    "    smt.df_to_kml(all_source_data[indices], 'layer %d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# twin certain zones to have megathrust\n",
    "if not any(all_source_data['tectonic subregion'] == 'subduction interface megathrust'):\n",
    "    indices = ((all_source_data['tectonic subregion'] == 'subduction interface')\n",
    "               & (all_source_data['mmax'] > 7.5))\n",
    "    sub_inter_zones = all_source_data[indices].copy()\n",
    "\n",
    "    all_source_data.loc[indices, 'mmax'] = 7.5\n",
    "    sub_inter_zones.loc[indices, 'mmin'] = 7.5\n",
    "    sub_inter_zones['tectonic subregion'] = 'subduction interface megathrust'\n",
    "    sub_inter_zones['zoneid'] = (sub_inter_zones['zoneid'] % 1000) + 1000\n",
    "    all_source_data = pd.concat([all_source_data, sub_inter_zones])\n",
    "    all_source_data = all_source_data.set_index(all_source_data['zoneid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add some binwise sesismicity rates for comparison\n",
    "for m in range(5, 8):\n",
    "    m_lo = np.maximum(all_source_data['mmin'], m)\n",
    "    m_hi = np.minimum(all_source_data['mmax'], m + 1)\n",
    "    log_N_m_lo = all_source_data['avalue'] - all_source_data['bvalue']*m_lo\n",
    "    log_N_m_hi = all_source_data['avalue'] - all_source_data['bvalue']*m_hi\n",
    "    log_N_bin = np.log10(10**log_N_m_lo - 10**log_N_m_hi).round(2)\n",
    "    series_name = 'logN_%.1f-%.1f' % (m, m +1)\n",
    "    all_source_data[series_name] = pd.Series(log_N_bin, index=all_source_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show a summary\n",
    "drop_columns = ['polygon coordinates', 'tectonic zone', 'region', \n",
    "                'concerns', 'layerid', 'zmin',\n",
    "                'aspect ratio', 'dip', 'rake', 'strike',\n",
    "                'logN_5.0-6.0', 'logN_6.0-7.0']\n",
    "pd.concat([all_source_data.drop(drop_columns, axis=1).head(),\n",
    "           all_source_data.drop(drop_columns, axis=1).tail()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a template to a file, to fill in missing info\n",
    "#keep = ['zoneid','zmin','zmax','strike','dip','rake']\n",
    "#for i, aux_file in zip(layer_ids, aux_files):\n",
    "#    layer_data = sources_data[sources_data['layerid'] == i]\n",
    "#    layer_data[keep].to_csv(aux_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write NRML source file\n",
    "sources = smt.df_to_source_list(all_source_data)\n",
    "source_model = src.source_model.mtkSourceModel(\n",
    "    identifier='1', name=model_path + ' areal', sources=sources)\n",
    "source_model.serialise_to_nrml(areal_source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write source data to TSV file\n",
    "all_source_data.to_csv(all_source_data_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PseudoCatalogue:\n",
    "    \"\"\"\n",
    "    ugly hack: construct pseudo-cataloge from average mechanisms\n",
    "    \"\"\"\n",
    "    def __init__(self, source_model, select_depth='all'):\n",
    "        data = []\n",
    "        oq_sources = source_model.convert_to_oqhazardlib(tom.PoissonTOM(1.0))\n",
    "        for source in oq_sources:\n",
    "            \n",
    "            longitude = np.mean(source.polygon.lons)\n",
    "            latitude = np.mean(source.polygon.lats)\n",
    "            strike = source.nodal_plane_distribution.data[0][1].strike\n",
    "            dip = source.nodal_plane_distribution.data[0][1].dip\n",
    "            rake = source.nodal_plane_distribution.data[0][1].rake\n",
    "            magnitude = source.get_min_max_mag()[1]*2.5\n",
    "            depth = source.hypocenter_distribution.data[0][1]\n",
    "            name = source.id\n",
    "            \n",
    "            if select_depth == 'all' or depth == select_depth:\n",
    "                data.append({'longitude': longitude, 'latitude': latitude,\n",
    "                             'strike1': strike, 'dip1': dip, 'rake1': rake,\n",
    "                             'magnitude': magnitude, 'depth': depth, 'id': name})\n",
    "        self.data = pd.DataFrame(data)\n",
    "\n",
    "    def get_number_tensors(self):\n",
    "        return len(self.data.magnitude)\n",
    "\n",
    "catalogue = PseudoCatalogue(source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_config = {\"min_lon\": 60, \"max_lon\": 105, \n",
    "              \"min_lat\": 0,  \"max_lat\": 40, \"resolution\": \"l\",\n",
    "              \"parallel_meridian_spacing\": 5}\n",
    "parser = nrmlSourceModelParser(areal_source_model)\n",
    "\n",
    "for depth in sorted(list(set(catalogue.data['depth']))):\n",
    "    basemap = HMTKBaseMap(map_config, '')\n",
    "\n",
    "    source_model_read = parser.read_file('Areal Source Model')\n",
    "    selected_sources = [source for source in source_model_read.sources \n",
    "                    if source.hypo_depth_dist.data[0][1] == depth]\n",
    "    source_model_read.sources = selected_sources    \n",
    "    selected_catalogue = PseudoCatalogue(source_model_read)\n",
    "\n",
    "    basemap.add_source_model(source_model_read, overlay=True) \n",
    "    basemap.add_focal_mechanism(selected_catalogue, magnitude=False)\n",
    "    for _, item in selected_catalogue.data.iterrows():\n",
    "        plt.annotate(s=item.id, xy=(item.longitude, item.latitude))\n",
    "    plt.savefig(\"ArealModel%gkmDepth.png\" % depth, dpi=300,\n",
    "                transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
