{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source models for Nath & Thingbaijam (2012)\n",
    "\n",
    "Read the source description input files from the online supplementary material and write them to XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%autoreload 2\n",
    "import source_model_tools as smt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toolbox as tb\n",
    "import lxml.etree as et\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from StringIO import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "import hmtk.sources as src\n",
    "from openquake.hazardlib import tom, geo\n",
    "\n",
    "from hmtk.plotting.mapping import HMTKBaseMap\n",
    "from hmtk.parsers.source_model.nrml04_parser import nrmlSourceModelParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some lists needed at different stages\n",
    "layer_ids = [1, 2, 3, 4]\n",
    "layer_depths_km = [0, 25, 70, 180, 300]\n",
    "\n",
    "min_mags = [4.5, 5.5]\n",
    "\n",
    "# define the input file names from the original paper\n",
    "model_path = 'nath2012probabilistic'\n",
    "polygon_file_template = os.path.join(model_path,'polygonlay%d.txt')\n",
    "seismicity_file_template = os.path.join(model_path,'seismicitylay%d.txt')\n",
    "polygon_files = [polygon_file_template % i for i in layer_ids] \n",
    "seismicity_files = [seismicity_file_template % i for i in layer_ids] \n",
    "\n",
    "smoothed_data_template = os.path.join(model_path,'lay%dsmooth%.1f.txt')\n",
    "smoothed_data_files = [[smoothed_data_template % (i, m) \n",
    "                        for i in layer_ids] for m in min_mags]\n",
    "\n",
    "# implement some column-name replacements\n",
    "variable_mapping = {'avalue': 'a', 'bvalue': 'b', 'stdbvalue': 'stdb'}\n",
    "\n",
    "# an input file supplies some auxiliary data\n",
    "aux_file = 'auxiliary data.csv'\n",
    "\n",
    "# define prefixes for the output file names and models\n",
    "areal_source_model_file = 'areal_source_model'\n",
    "smoothed_source_data_file = 'smoothed_source_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_polygons(file_name):\n",
    "    \"\"\"\n",
    "    Read polygon descriptions from text file into pandas.DataFrame. \n",
    "    \n",
    "    File format as per Nath & Thingbaijam (2012).\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        keys = f.readline().strip().split(',')\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            values = line.strip().split(',', len(keys) - 1)\n",
    "            entry = {}\n",
    "            for key, value in zip(keys, values):\n",
    "                key = key.strip('[]')\n",
    "                value = value.strip('[]').replace(';', '\\n')\n",
    "                if key == 'zoneid':\n",
    "                    value = int(value)\n",
    "                else:\n",
    "                    value = np.genfromtxt(StringIO(value), delimiter=',')\n",
    "                entry[key] = value\n",
    "            result.append(entry)\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read areal polygons and seismicity statistics for each layer\n",
    "areal_df = pd.DataFrame()\n",
    "areal_polygons_df = pd.DataFrame()\n",
    "loop_data = zip(layer_ids, seismicity_files, polygon_files, \n",
    "                layer_depths_km[:-1], layer_depths_km[1:])\n",
    "for layer_id, seismicity_file, polygon_file, z_min, z_max in loop_data:\n",
    "        \n",
    "    # read seismicity and polygons\n",
    "    layer_seis_df = pd.read_csv(seismicity_file)\n",
    "    layer_seis_df.rename(columns=variable_mapping, inplace=True)\n",
    "    layer_poly_df = read_polygons(polygon_file)\n",
    "    \n",
    "    # fill in depths, specify source mechanisms, clean up dip & rake\n",
    "    n_zones = len(layer_seis_df)\n",
    "    idx = layer_seis_df.index\n",
    "    layer_seis_df['zmin'] = pd.Series(np.full(n_zones, z_min), index=idx)\n",
    "    layer_seis_df['zmax'] = pd.Series(np.full(n_zones, z_max), index=idx)\n",
    "    layer_seis_df['layerid'] = pd.Series(np.full(n_zones, layer_id), index=idx)\n",
    "    layer_seis_df['rake'] = tb.wrap(layer_seis_df['rake'])\n",
    "    layer_seis_df['mechanism'] = pd.Series(\n",
    "        smt.focal_mechanisms(layer_seis_df['dip'], layer_seis_df['rake']), index=idx)\n",
    "    layer_seis_df.loc[layer_seis_df['dip'] == -1, 'dip'] = 45\n",
    "    layer_seis_df.loc[layer_seis_df['mechanism'] == 'undefined', 'rake'] = 90\n",
    "    layer_seis_df.loc[layer_seis_df['strike'] == -1, 'strike'] = 0\n",
    "    layer_seis_df['mmin'] = pd.Series(np.full(n_zones, min_mags[0]), index=idx) \n",
    "    \n",
    "    # put it all together\n",
    "    layer_source_df = pd.merge(layer_seis_df, layer_poly_df, on='zoneid')\n",
    "    areal_df = pd.concat([areal_df, layer_source_df], ignore_index=True)\n",
    "    areal_polygons_df = pd.concat([areal_polygons_df, layer_poly_df], ignore_index=True)\n",
    "\n",
    "# merge auxiliary data (crucially, tectonic zones)\n",
    "aux_df = pd.read_csv(aux_file)\n",
    "aux_df = aux_df.drop(['zmin','zmax','dip','rake','mechanism'], axis=1)\n",
    "areal_df = pd.merge(areal_df, aux_df, on='zoneid')\n",
    "areal_df['polygon coordinates'] = areal_df['polygon coordinates'].astype(np.ndarray)\n",
    "\n",
    "# convert polygons coordinates to objects\n",
    "areal_df['polygon'] = [\n",
    "    geo.polygon.Polygon([geo.point.Point(lat, lon) \n",
    "                         for lat, lon in area_series['polygon coordinates']]) \n",
    "    for _, area_series in areal_df.iterrows()]\n",
    "\n",
    "areal_df = smt.sort_and_reindex(areal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show a summary including megathrust zones and bin statistics\n",
    "drop_columns = ['tectonic zone', 'region', 'concerns', 'zmax', 'zmin',\n",
    "                'polygon coordinates', 'polygon',\n",
    "                'aspect ratio', 'dip', 'rake', 'strike']\n",
    "display(pd.concat([areal_df.drop(drop_columns, axis=1).head(),\n",
    "                   areal_df.drop(drop_columns, axis=1).tail()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "props = ['a','b','mmax']\n",
    "ranges = [np.arange(-0.5, 8, 1), np.arange(-0.1, 1.7, 0.2), np.arange(-0.5, 10, 1)]\n",
    "groups = areal_df.groupby('layerid')\n",
    "fig, axes = plt.subplots(nrows=len(props), ncols=1, figsize=(6, 3*len(props)))\n",
    "for prop, ax, bins in zip(props, axes, ranges):\n",
    "    data = [group[prop] for _, group in groups]\n",
    "    labels = ['layer %d' % id for id, _ in groups]\n",
    "    ax.hist(data, label=labels, stacked=True, bins=bins)\n",
    "    ax.set_ylabel(prop)\n",
    "axes[0].legend(loc='upper left');\n",
    "plt.savefig(\"ArealModelFmds.png\", dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_df[areal_df['tectonic subregion'] == 'no seismicity'].drop(['polygon coordinates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read smoothed seismicity data\n",
    "smoothed_df_list = []\n",
    "for mag, file_names in zip(min_mags, smoothed_data_files):\n",
    "    \n",
    "    layer_smoothed_df_list = []\n",
    "    for layer_id, smoothed_file in zip(layer_ids, file_names):\n",
    "        layer_smoothed_df = pd.read_csv(smoothed_file)\n",
    "        nu_mag = 'nu%s' % str(mag).replace('.','_')\n",
    "\n",
    "        renaming_dict = {nu_mag: 'nu', 'lat':'latitude', 'lon':'longitude'}\n",
    "        layer_smoothed_df.rename(columns=renaming_dict, inplace=True)\n",
    "        \n",
    "        layer_smoothed_df['layerid'] = layer_id\n",
    "        layer_smoothed_df['mmin model'] = mag\n",
    "        layer_smoothed_df['mmin'] = mag\n",
    "\n",
    "        layer_smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "    layer_smoothed_df = pd.concat(layer_smoothed_df_list)\n",
    "    smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "smoothed_df = pd.concat(smoothed_df_list)\n",
    "smoothed_df = smt.sort_and_reindex(smoothed_df)\n",
    "\n",
    "display(pd.concat((smoothed_df.head(), smoothed_df.tail())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop = 'nu'\n",
    "fig, axes = plt.subplots(nrows=len(min_mags), ncols=1, figsize=(6, 3*len(min_mags)), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.1)\n",
    "for min_mag, ax in zip(min_mags, axes):\n",
    "    groups = smoothed_df[smoothed_df['mmin model'] == min_mag].groupby('layerid')\n",
    "    data = [np.log10(group[prop]).values for _, group in groups]\n",
    "    labels = ['layer %d' % id for id, _ in groups]\n",
    "    ax.hist(data, label=labels, stacked=True, bins=np.arange(-4,1,0.5))\n",
    "    ax.set_ylabel(('%s%g' % (prop, min_mag)).replace('.','_'))\n",
    "axes[0].legend(loc='upper left')\n",
    "plt.savefig(\"SmoothedModelActivityRates.png\", dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute all distances\n",
    "distances = np.full((len(smoothed_df), len(areal_df)), np.inf)\n",
    "for i, area_series in areal_df.iterrows():\n",
    "    at_depth = (smoothed_df['layerid'] == area_series['layerid']).values\n",
    "    mesh = geo.mesh.Mesh(\n",
    "        smoothed_df.loc[at_depth, 'longitude'].values,\n",
    "        smoothed_df.loc[at_depth, 'latitude'].values)\n",
    "    distances[at_depth, i] = area_series['polygon'].distances(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truncated Gutenberg-Richter magnitude-frequency distribution in OpenQuake implements\n",
    "$$\\lambda(m \\geq M) = 10^{a - b m} = e^{\\alpha - \\beta m}$$\n",
    "where, since $\\lambda$ is an annual rate, $10^a$ is too. If we ignore events below some threshold $m_{min}$ then the annual rate becomes\n",
    "$$\\lambda(m \\geq m_{min}) = e^{\\alpha - \\beta m_{min}} e^{-\\beta (m - m_{min})} = \\nu e^{-\\beta (m - m_{min})} $$\n",
    "Thus to compute the $a$ value required by OpenQuake from the activity rate $\\nu$ for a given magnitude threshold, we must also take into account the $b$ value for the zone:\n",
    "$$a = \\log_{10}(\\nu) + b m_{min}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_model = smoothed_df['mmin model'] == 4.5\n",
    "n_points_in_area = (distances == 0).sum(axis=0)\n",
    "zone_id = 21\n",
    "drop_cols = ['tectonic subregion','region', 'tectonic zone',\n",
    "             'aspect ratio', 'polygon coordinates']\n",
    "fmd_cols = ['zoneid','layerid','a','b','stdb','mmax','stdmmax']\n",
    "zone_index = areal_df[areal_df['zoneid'] == zone_id].index\n",
    "print n_points_in_area[zone_index][0]\n",
    "display(areal_df[areal_df['zoneid'] == zone_id][fmd_cols])\n",
    "print areal_df[areal_df['zoneid'] == zone_id]['b'].values[0]*5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each point in the smoothed model, choose the closest areal zone \n",
    "# and copy some useful columns over\n",
    "columns_to_copy = ['zoneid', 'zmax', 'zmin', 'tectonic subregion',\n",
    "                   'a', 'b', 'stdb', 'mmax', 'stdmmax',\n",
    "                   'rake', 'dip', 'strike', 'aspect ratio', 'msr']\n",
    "index_min = distances.argmin(axis=1)\n",
    "for i, area_series in areal_df.iterrows():\n",
    "    picked = i == index_min\n",
    "    for column in columns_to_copy:\n",
    "        smoothed_df.loc[picked, column] = area_series[column]\n",
    "    # grab mmax and bvalue from zone above if mmax zero for this zone\n",
    "    if area_series['mmax'] == 0:\n",
    "        alternate_zone = np.floor(area_series['zoneid']/10)\n",
    "        index_alt = np.where(areal_df['zoneid'] == alternate_zone)[0]\n",
    "        if len(index_alt) > 0:\n",
    "            smoothed_df.loc[picked, 'mmax'] = areal_df.loc[index_alt[0], 'mmax']\n",
    "            smoothed_df.loc[picked, 'b'] = areal_df.loc[index_alt[0], 'b']\n",
    "\n",
    "# computing the a-value for each zone is now a cinch\n",
    "smoothed_df['a zone'] = smoothed_df['a'].copy()\n",
    "smoothed_df['a'] = smoothed_df['nu'] + smoothed_df['b']*smoothed_df['mmin model']\n",
    "\n",
    "# for each area in the areal model: count the number of points and \n",
    "# sum the activity rates in the smoothed model. from the latter estimate\n",
    "# an equivalent a-value\n",
    "for mag in min_mags:\n",
    "    this_model = smoothed_df['mmin model'] == mag\n",
    "    sum_nu = np.array([smoothed_df.loc[(distance == 0) & this_model.values, 'nu'].sum()\n",
    "          for distance in distances.T])\n",
    "    areal_df['smoothed N ' +  str(mag)] = (distances[this_model.values, :] == 0).sum(axis=0)\n",
    "    areal_df['smoothed nu ' +  str(mag)] = sum_nu.round(4)\n",
    "    areal_df['smoothed a ' +  str(mag)] = (np.log10(nu_smoothed) + areal_df['b']*mag).round(2)\n",
    "    areal_df['areal nu ' +  str(mag)] = (10**(areal_df['a'] - areal_df['b']*mag)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_drop = ['zmax', 'zmin', 'aspect ratio', 'msr',\n",
    "                'rake','dip','strike','stdb','stdmmax']\n",
    "no_mmax_df = smoothed_df[smoothed_df['mmax'] == 0]\n",
    "if len(no_mmax_df) > 0:\n",
    "    print \"Leftover points with no assigned mmax\"\n",
    "    display(no_mmax_df.drop(display_drop, axis=1).head())\n",
    "no_b_df = smoothed_df[smoothed_df['b'] == 0]\n",
    "if len(no_b_df) > 0:\n",
    "    print \"Leftover points with no assigned b\"\n",
    "    display(no_b_df.drop(display_drop, axis=1).head())\n",
    "no_zoneid_df = smoothed_df[smoothed_df['zoneid'].isnull()]\n",
    "if len(no_zoneid_df) > 0:\n",
    "    print \"Leftover points with no assigned zone id\"\n",
    "    display(no_zoneid_df.drop(display_drop, axis=1).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((smoothed_df.head(), smoothed_df.tail())).drop(display_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_lon_lat = (smoothed_df['longitude'] == 98) & (smoothed_df['latitude'] == 3.7)\n",
    "display(smoothed_df[this_lon_lat].drop(display_drop, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_display_drop = display_drop + ['polygon coordinates','tectonic zone','concerns','layerid']\n",
    "pd.concat((areal_df.head(), areal_df.tail())).drop(areal_display_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nu_df = areal_df[['zoneid', 'layerid', 'areal nu 4.5', 'areal nu 5.5', \n",
    "                  'smoothed nu 4.5', 'smoothed nu 5.5', 'smoothed N 4.5', 'smoothed N 5.5']]\n",
    "nu_df = nu_df.rename(columns={'areal nu 4.5': 'areal 4.5', 'smoothed nu 4.5': 'smoothed 4.5',\n",
    "                              'areal nu 5.5': 'areal 5.5', 'smoothed nu 5.5': 'smoothed 5.5',\n",
    "                              'smoothed N 4.5': 'N 4.5', 'smoothed N 5.5': 'N 5.5'})\n",
    "\n",
    "for layer_id in layer_ids:\n",
    "    series = pd.Series(nu_df[nu_df['layerid'] == layer_id].sum(axis=0))\n",
    "    series['layerid'] = layer_id\n",
    "    series['zoneid'] = 'All'\n",
    "    nu_df = nu_df.append(series, ignore_index=True)\n",
    "\n",
    "series = pd.Series(nu_df[nu_df['zoneid'] == 'All'].sum(axis=0))\n",
    "series['layerid'] = 'All'\n",
    "series['zoneid'] = 'All'\n",
    "nu_df = nu_df.append(series, ignore_index=True)\n",
    "nu_df['ratio 4.5'] = (nu_df['smoothed 4.5']/nu_df['areal 4.5']).round(2)\n",
    "nu_df['ratio 5.5'] = (nu_df['smoothed 5.5']/nu_df['areal 5.5']).round(2)\n",
    "\n",
    "random_zones = sorted(np.random.randint(len(areal_df), size=5))\n",
    "pd.concat((nu_df.loc[random_zones, :], nu_df[nu_df['zoneid'] == 'All']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try computing layers directly\n",
    "nu_df = pd.DataFrame()\n",
    "nu_df.index.name = 'layerid'\n",
    "\n",
    "for layer_id in layer_ids:\n",
    "    in_areal_layer = areal_df['layerid'] == layer_id\n",
    "    in_smoothed_layer = smoothed_df['layerid'] == layer_id\n",
    "    \n",
    "    layer_series = pd.Series()\n",
    "    for mag in min_mags:\n",
    "\n",
    "        this_model = smoothed_df['mmin model'] == mag\n",
    "        layer_series = layer_series.append(pd.Series({\n",
    "            'areal ' + str(mag): (10**(areal_df[in_areal_layer]['a'] -  areal_df[in_areal_layer]['b']*mag)).sum().round(),\n",
    "            'smoothed ' + str(mag): smoothed_df[in_smoothed_layer & this_model]['nu'].sum().round(), \n",
    "            }, name=layer_id))\n",
    "    nu_df = nu_df.append(layer_series)\n",
    "\n",
    "nu_df = nu_df.append(pd.Series(nu_df.sum(axis=0), name='Total'))\n",
    "for mag in min_mags:\n",
    "    nu_df['ratio ' + str(mag)] = (nu_df['smoothed ' + str(mag)]/nu_df['areal ' + str(mag)]).round()\n",
    "\n",
    "display(nu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare to the catalogue\n",
    "catalogue_df = pd.read_csv('../Catalogue/SACAT1900_2008v2.txt', sep='\\t')\n",
    "completeness_df = pd.read_csv('./thingbaijam2011seismogenic/Table1.csv', header=[0,1], index_col=[0,1])\n",
    "completeness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catalogue_df['SHOCK_TYPE'].value_counts().plot(kind='pie', figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign zones to events\n",
    "catalogue_df['zoneid'] = -1\n",
    "for i, area_series in areal_df.iterrows():\n",
    "    at_depth = ((catalogue_df['DEPTH'] >= area_series['zmin']) & \n",
    "                (catalogue_df['DEPTH'] < area_series['zmax']))\n",
    "    mesh = geo.mesh.Mesh(catalogue_df['LON'].values,\n",
    "                         catalogue_df['LAT'].values)\n",
    "    in_area = area_series['polygon'].distances(mesh) == 0\n",
    "    catalogue_df.loc[at_depth & in_area, 'zoneid'] = area_series['zoneid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((catalogue_df.head(), catalogue_df.tail()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each minimum magnitude and layer work out the activity rates\n",
    "nu_cat_df = pd.DataFrame()\n",
    "for layer_id, z_min, z_max in reversed(zip(layer_ids, layer_depths_km[:-1], layer_depths_km[1:])):\n",
    "    layer_results = pd.Series()\n",
    "    for mag in reversed(min_mags):\n",
    "        above_thresh = catalogue_df['MAG_MW'] >= mag\n",
    "        start = completeness_df[str(mag), 'start'][z_min, z_max]\n",
    "        end = completeness_df[str(mag), 'end'][z_min, z_max]\n",
    "        at_depth = ((catalogue_df['DEPTH'] >= z_min) & \n",
    "                    (catalogue_df['DEPTH'] < z_max))\n",
    "        in_years = ((catalogue_df['YEAR'] >= start) & \n",
    "                    (catalogue_df['YEAR'] <= end))\n",
    "        in_a_zone = catalogue_df['zoneid'] != -1\n",
    "        is_mainshock = catalogue_df['SHOCK_TYPE'] == 'Mainshock'\n",
    "        subcat_df = catalogue_df[above_thresh & at_depth & in_years & in_a_zone & is_mainshock]\n",
    "        layer_results = layer_results.append(pd.Series({\n",
    "                'cat ' + str(mag): len(subcat_df)/(end - start + 1),\n",
    "            }, name=layer_id))\n",
    "    nu_cat_df = nu_cat_df.append(layer_results)\n",
    "nu_cat_df = nu_cat_df.append(pd.Series(nu_cat_df.sum(axis=0), name='Total'))\n",
    "display(nu_df.join(nu_cat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((subcat_df.head(), subcat_df.tail()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = 85  # km correlation distance\n",
    "delta = 0.1  # grid spacing\n",
    "span = 2.5  # test grid span\n",
    "lon = 80  # degrees\n",
    "lat = 20  # degrees\n",
    "point = geo.Point(lon, lat)\n",
    "lon_list = np.arange(lon - span, lon + span + delta/2, delta)\n",
    "lat_list = np.arange(lat - span, lat + span + delta/2, delta)\n",
    "lon_limits = (lon - span - delta/2, lon + span + delta/2)\n",
    "lat_limits = (lat - span - delta/2, lat + span + delta/2)\n",
    "lon_mesh, lat_mesh = np.meshgrid(lon_list, lat_list)\n",
    "mesh = geo.RectangularMesh(lon_mesh, lat_mesh)\n",
    "distances = point.distance_to_mesh(mesh)\n",
    "i_mid = int(span/delta)\n",
    "span_km = (distances[i_mid,:].max(), distances[:,i_mid].max())\n",
    "print('Grid extents %g and %g km' % span_km)\n",
    "print('Must be at least 3x correlation distance or %g km' % (3*c))\n",
    "if all(np.array(span_km) > 3*c):\n",
    "    print 'OK!'\n",
    "else:\n",
    "    print 'Problem!'\n",
    "weights = np.exp(-(distances/c)**2) * (distances < 3*c)\n",
    "print(u'For correlation distance %g km and grid spacing %g°' % (c, delta))\n",
    "print('the unnormalized weights sum to %g.' % weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(distances, origin='lower', aspect='equal', \n",
    "           interpolation='nearest', \n",
    "           extent=lon_limits + lat_limits)\n",
    "plt.title('Test grid to compute smoothing normalization')\n",
    "plt.xlabel(u'Longitude (°E)')\n",
    "plt.ylabel(u'Latitude (°N)')\n",
    "plt.grid()\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(weights, cmap='hot', origin='lower', aspect='equal', \n",
    "           interpolation='nearest', \n",
    "           extent=lon_limits + lat_limits)\n",
    "plt.title('Weights on test grid')\n",
    "plt.xlabel(u'Longitude (°E)')\n",
    "plt.ylabel(u'Latitude (°N)')\n",
    "plt.grid()\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_mag = 5.5\n",
    "layer_id = 1\n",
    "model_layer = (smoothed_df['mmin model'] == min_mag) & \\\n",
    "    (smoothed_df['layerid'] == layer_id)\n",
    "subset_df = smoothed_df[model_layer].copy()\n",
    "\n",
    "lon_list = sorted(list(set(subset_df['longitude'])))\n",
    "lat_list = sorted(list(set(subset_df['latitude'])))\n",
    "\n",
    "lon_min, lon_max = [min(lon_list), max(lon_list)]\n",
    "lat_min, lat_max = [min(lat_list), max(lat_list)]\n",
    "lon_res = np.diff(lon_list).min().round(2)\n",
    "lat_res = np.diff(lat_list).min().round(2)\n",
    "\n",
    "lon_list = np.arange(lon_min, lon_max + lon_res, lon_res).round(2)\n",
    "lat_list = np.arange(lat_min, lat_max + lat_res, lat_res).round(2)\n",
    "lat_grid, lon_grid = np.meshgrid(lat_list, lon_list)\n",
    "\n",
    "# assign known values\n",
    "num_columns = ['a', 'b', 'nu']\n",
    "txt_columns = ['tectonic subregion']\n",
    "data = {}\n",
    "for column in num_columns:\n",
    "    data[column] = np.full_like(lon_grid, np.nan)\n",
    "for column in txt_columns:\n",
    "    data[column] = np.full_like(lon_grid, '', dtype='object')\n",
    "\n",
    "for _, point_series in subset_df.iterrows():\n",
    "    i = int(round((point_series['longitude'] - lon_min)/lon_res))\n",
    "    j = int(round((point_series['latitude'] - lat_min)/lat_res))\n",
    "    for column in num_columns:\n",
    "        data[column][i, j] = point_series[column]\n",
    "    for column in txt_columns:\n",
    "        data[column][i, j] = point_series[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = 'a'\n",
    "if param in ['nu']:\n",
    "    log_scale = True\n",
    "else:\n",
    "    log_scale = False\n",
    "limits = (np.nanmin(data[param]), np.nanmax(data[param]))\n",
    "plt.hist(data[param].ravel(), range=limits, log=log_scale);\n",
    "plt.savefig(\"SmoothedEquivalentMap_%s_mmin%g_layer%d.png\" % (param, min_mag, layer_id), dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca()\n",
    "if log_scale:\n",
    "    plt.imshow(data[param], cmap='hot', origin='lower', aspect='equal', \n",
    "               extent=(lon_min, lon_max, lat_min, lat_max), \n",
    "               norm=LogNorm(vmin=limits[0], vmax=limits[1]))\n",
    "else:\n",
    "    plt.imshow(data[param], cmap='hot', origin='lower', aspect='equal', \n",
    "               extent=(lon_min, lon_max, lat_min, lat_max))\n",
    "\n",
    "plt.colorbar(shrink=0.5)\n",
    "ax.set_xlabel(u'Longitude (°E)')\n",
    "ax.set_ylabel(u'Latitude (°N)')\n",
    "ax.grid()\n",
    "plt.savefig(\"SmoothedEquivalentMap_%s_mmin%g_layer%d.png\" % (param, min_mag, layer_id), dpi=300,\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write each layer of areal source model to KML with added binwise rates\n",
    "areal_kml_df = smt.add_binwise_rates(areal_df)\n",
    "for layer_id in layer_ids:\n",
    "    this_layer = areal_kml_df['layerid'] == layer_id\n",
    "    temp_df = areal_kml_df.drop(['layerid'], axis=1)\n",
    "    smt.source_df_to_kml(temp_df.loc[this_layer, :], \n",
    "        '%s layer %d' % (areal_source_model_file, layer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thin smoothed DataFrame so that resulting KML files are small enough to be read into QGIS\n",
    "smoothed_kml_df = smoothed_df.copy()\n",
    "drop_columns = ['zmax','zmin','aspect ratio','rake','dip','strike',\n",
    "                'msr','stdb','stdmmax']\n",
    "smoothed_kml_df.drop(drop_columns, axis=1, inplace=True)\n",
    "res_deg = 0.2\n",
    "this_lon_lat = ((((smoothed_kml_df['latitude'] % res_deg).round(2) % res_deg) == 0) &\n",
    "                (((smoothed_kml_df['longitude'] % res_deg).round(2) % res_deg) == 0))\n",
    "smoothed_kml_df = smoothed_kml_df[this_lon_lat]\n",
    "\n",
    "# write each layer to KML with added binwise rates\n",
    "for min_mag in reversed(min_mags):\n",
    "    for layer_id in reversed(layer_ids):\n",
    "        this_model_layer = ((smoothed_kml_df['mmin model'] == min_mag) &\n",
    "                   (smoothed_kml_df['layerid'] == layer_id))\n",
    "        temp_df = smoothed_kml_df.drop(['mmin model','layerid'], axis=1)\n",
    "        smt.source_df_to_kml(temp_df.loc[this_model_layer, :], \n",
    "            '%s mmin %g layer %d' % (smoothed_source_data_file, min_mag, layer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write areal model data to TSV file\n",
    "areal_output_df = smt.add_binwise_rates(\n",
    "    smt.twin_source_by_magnitude(areal_df))\n",
    "areal_output_df.to_csv(areal_source_model_file + '.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write areal source model with megathrust sources twinned to NRML\n",
    "areal_source_list = smt.source_df_to_list(\n",
    "    smt.twin_source_by_magnitude(areal_df))\n",
    "areal_source_model = src.source_model.mtkSourceModel(\n",
    "    identifier='1', \n",
    "    name=model_path + ' areal', \n",
    "    sources=areal_source_list)\n",
    "areal_source_model.serialise_to_nrml(areal_source_model_file + '.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write point source models with megathrust sources twinned to NRML\n",
    "for min_mag in min_mags:\n",
    "    this_model = smoothed_df['mmin'] == min_mag \n",
    "    smoothed_source_list = smt.source_df_to_list(\n",
    "        smt.twin_source_by_magnitude(smoothed_df.loc[this_model, :]))\n",
    "    smoothed_source_model = src.source_model.mtkSourceModel(\n",
    "        identifier='1', \n",
    "        name='%s smoothed m_min=%g' % (model_path, min_mag), \n",
    "        sources=smoothed_source_list)\n",
    "    file_name = smoothed_source_data_file + '_mmin%g.xml' % min_mag \n",
    "    smoothed_source_model.serialise_to_nrml(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PseudoCatalogue:\n",
    "    \"\"\"\n",
    "    ugly hack for plotting source mechanisms: \n",
    "    construct pseudo-cataloge from pandas.DataFrame\n",
    "    \"\"\"\n",
    "    def __init__(self, source_model, select_depth='all'):\n",
    "        data = []\n",
    "        oq_sources = source_model.convert_to_oqhazardlib(tom.PoissonTOM(1.0))\n",
    "        for source in oq_sources:\n",
    "            \n",
    "            longitude = np.mean(source.polygon.lons)\n",
    "            latitude = np.mean(source.polygon.lats)\n",
    "            strike = source.nodal_plane_distribution.data[0][1].strike\n",
    "            dip = source.nodal_plane_distribution.data[0][1].dip\n",
    "            rake = source.nodal_plane_distribution.data[0][1].rake\n",
    "            magnitude = source.get_min_max_mag()[1]*2.5\n",
    "            depth = source.hypocenter_distribution.data[0][1]\n",
    "            name = source.id\n",
    "            \n",
    "            if select_depth == 'all' or depth == select_depth:\n",
    "                data.append({'longitude': longitude, 'latitude': latitude,\n",
    "                             'strike1': strike, 'dip1': dip, 'rake1': rake,\n",
    "                             'magnitude': magnitude, 'depth': depth, 'id': name})\n",
    "        self.data = pd.DataFrame(data)\n",
    "\n",
    "    def get_number_tensors(self):\n",
    "        return len(self.data.magnitude)\n",
    "\n",
    "catalogue = PseudoCatalogue(areal_source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_config = {\"min_lon\": 60, \"max_lon\": 105, \n",
    "              \"min_lat\": 0,  \"max_lat\": 40, \"resolution\": \"l\",\n",
    "              \"parallel_meridian_spacing\": 5}\n",
    "parser = nrmlSourceModelParser(areal_source_model_file + '.xml')\n",
    "\n",
    "for depth in sorted(list(set(catalogue.data['depth']))):\n",
    "    basemap = HMTKBaseMap(map_config, '')\n",
    "\n",
    "    source_model_read = parser.read_file('Areal Source Model')\n",
    "    selected_sources = [source for source in source_model_read.sources \n",
    "                    if source.hypo_depth_dist.data[0][1] == depth]\n",
    "    source_model_read.sources = selected_sources    \n",
    "    selected_catalogue = PseudoCatalogue(source_model_read)\n",
    "\n",
    "    basemap.add_source_model(source_model_read, overlay=True) \n",
    "    basemap.add_focal_mechanism(selected_catalogue, magnitude=False)\n",
    "    for _, item in selected_catalogue.data.iterrows():\n",
    "        plt.annotate(s=item.id, xy=(item.longitude, item.latitude))\n",
    "    plt.savefig(\"ArealModel%gkmDepth.png\" % depth, dpi=300,\n",
    "                transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
