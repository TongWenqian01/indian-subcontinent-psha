{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source models for Nath & Thingbaijam (2012)\n",
    "\n",
    "Read the source description input files from the online supplementary material and write them to XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%autoreload 2\n",
    "import source_model_tools as smt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toolbox as tb\n",
    "import lxml.etree as et\n",
    "import matplotlib.pyplot as plt\n",
    "from StringIO import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "import hmtk.sources as src\n",
    "from openquake.hazardlib import tom, geo\n",
    "\n",
    "from hmtk.plotting.mapping import HMTKBaseMap\n",
    "from hmtk.parsers.source_model.nrml04_parser import nrmlSourceModelParser\n",
    "\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_ids = [1, 2, 3, 4]\n",
    "layer_depths_km = [0, 25, 70, 180, 300]\n",
    "min_mags = [4.5, 5.5]\n",
    "\n",
    "model_path = 'nath2012probabilistic'\n",
    "polygon_file_template = os.path.join(model_path,'polygonlay%d.txt')\n",
    "seismicity_file_template = os.path.join(model_path,'seismicitylay%d.txt')\n",
    "polygon_files = [polygon_file_template % i for i in layer_ids] \n",
    "seismicity_files = [seismicity_file_template % i for i in layer_ids] \n",
    "\n",
    "aux_file = 'auxiliary data.csv'\n",
    "\n",
    "smoothed_data_template = os.path.join(model_path,'lay%dsmooth%.1f.txt')\n",
    "smoothed_data_files = [[smoothed_data_template % (i, m) \n",
    "                        for i in layer_ids] for m in min_mags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_polygons(file_name):\n",
    "    \"\"\"\n",
    "    Read polygon descriptions from text file into pandas.DataFrame. \n",
    "    \n",
    "    File format as per Nath & Thingbaijam (2012).\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        keys = f.readline().strip().split(',')\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            values = line.strip().split(',', len(keys) - 1)\n",
    "            entry = {}\n",
    "            for key, value in zip(keys, values):\n",
    "                key = key.strip('[]')\n",
    "                value = value.strip('[]').replace(';', '\\n')\n",
    "                if key == 'zoneid':\n",
    "                    value = int(value)\n",
    "                else:\n",
    "                    value = np.genfromtxt(StringIO(value), delimiter=',')\n",
    "                entry[key] = value\n",
    "            result.append(entry)\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read areal polygons and seismicity statistics for each layer\n",
    "areal_source_df = pd.DataFrame()\n",
    "areal_polygons_df = pd.DataFrame()\n",
    "loop_data = zip(layer_ids, seismicity_files, polygon_files, \n",
    "                layer_depths_km[:-1], layer_depths_km[1:])\n",
    "for layer_id, seismicity_file, polygon_file, z_min, z_max in loop_data:\n",
    "        \n",
    "    # read seismicity and polygons\n",
    "    layer_seis_df = pd.read_csv(seismicity_file)\n",
    "    layer_poly_df = read_polygons(polygon_file)\n",
    "    \n",
    "    # fill in depths, specify source mechanisms, clean up dip & rake\n",
    "    n_zones = len(layer_seis_df)\n",
    "    idx = layer_seis_df.index\n",
    "    layer_seis_df['zmin'] = pd.Series(np.full(n_zones, z_min), index=idx)\n",
    "    layer_seis_df['zmax'] = pd.Series(np.full(n_zones, z_max), index=idx)\n",
    "    layer_seis_df['layerid'] = pd.Series(np.full(n_zones, layer_id), index=idx)\n",
    "    layer_seis_df['rake'] = tb.wrap(layer_seis_df['rake'])\n",
    "    layer_seis_df['mechanism'] = pd.Series(\n",
    "        smt.focal_mechanisms(layer_seis_df['dip'], layer_seis_df['rake']), index=idx)\n",
    "    layer_seis_df.loc[layer_seis_df['dip'] == -1, 'dip'] = 45\n",
    "    layer_seis_df.loc[layer_seis_df['mechanism'] == 'undefined', 'rake'] = 90\n",
    "    layer_seis_df.loc[layer_seis_df['strike'] == -1, 'strike'] = 0\n",
    "    layer_seis_df['mmin'] = pd.Series(np.full(n_zones, min_mags[0]), index=idx) \n",
    "    \n",
    "    # put it all together\n",
    "    layer_source_df = pd.merge(layer_seis_df, layer_poly_df, on='zoneid')\n",
    "    areal_source_df = pd.concat([areal_source_df, layer_source_df], ignore_index=True)\n",
    "    areal_polygons_df = pd.concat([areal_polygons_df, layer_poly_df], ignore_index=True)\n",
    "\n",
    "# merge auxiliary data (crucially, tectonic zones)\n",
    "aux_df = pd.read_csv(aux_file)\n",
    "aux_df = aux_df.drop(['zmin','zmax','dip','rake','mechanism'], axis=1)\n",
    "areal_source_df = pd.merge(areal_source_df, aux_df, on='zoneid')\n",
    "\n",
    "areal_source_df = smt.sort_and_reindex(areal_source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_source_model_file = 'areal_source_model'\n",
    "# write everything to TSV file\n",
    "areal_output_df = smt.add_binwise_rates(\n",
    "    smt.twin_source_by_magnitude(areal_source_df))\n",
    "areal_output_df.to_csv(areal_source_model_file + '.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write NRML source file with added megathrust zones\n",
    "areal_source_list = smt.source_df_to_list(\n",
    "    smt.twin_source_by_magnitude(areal_source_df))\n",
    "areal_source_model = src.source_model.mtkSourceModel(\n",
    "    identifier='1', \n",
    "    name=model_path + ' areal', \n",
    "    sources=areal_source_list)\n",
    "areal_source_model.serialise_to_nrml(areal_source_model_file + '.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write each layer to KML with added binwise rates\n",
    "areal_kml_df = smt.add_binwise_rates(areal_source_df)\n",
    "for layer_id in layer_ids:\n",
    "    indices = areal_kml_df['layerid'] == layer_id\n",
    "    temp_df = areal_kml_df.drop(['layerid'], axis=1)\n",
    "    smt.source_df_to_kml(temp_df.loc[indices,:], \n",
    "        '%s layer %d' % (areal_source_model_file, layer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show a summary including megathrust zones and bin statistics\n",
    "drop_columns = ['polygon coordinates', 'tectonic zone', 'region', \n",
    "                'concerns', 'zmax', 'zmin',\n",
    "                'aspect ratio', 'dip', 'rake', 'strike',\n",
    "                'logN_5.0-6.0', 'logN_6.0-7.0']\n",
    "display(pd.concat([areal_output_df.drop(drop_columns, axis=1).head(),\n",
    "                   areal_output_df.drop(drop_columns, axis=1).tail()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "areal_source_df[areal_source_df['tectonic subregion'] == 'no seismicity'].drop(['polygon coordinates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read smoothed seismicity data\n",
    "smoothed_df_list = []\n",
    "for mag, file_names in zip(min_mags, smoothed_data_files):\n",
    "    \n",
    "    layer_smoothed_df_list = []\n",
    "    for layer_id, smoothed_file in zip(layer_ids, file_names):\n",
    "        layer_smoothed_df = pd.read_csv(smoothed_file)\n",
    "        #layer_smoothed_df.rename(columns={'nu4_5':'nu'}, inplace=True)\n",
    "        #layer_smoothed_df.rename(columns={'nu5_5':'nu'}, inplace=True)\n",
    "        layer_smoothed_df.rename(columns={'lat':'latitude'}, inplace=True)\n",
    "        layer_smoothed_df.rename(columns={'lon':'longitude'}, inplace=True)\n",
    "        \n",
    "        # figure out what area each point is in\n",
    "        layer_smoothed_df['layerid'] = layer_id\n",
    "        layer_smoothed_df['mmin'] = mag\n",
    "        layer_smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "    layer_smoothed_df = pd.concat(layer_smoothed_df_list)\n",
    "    smoothed_df_list.append(layer_smoothed_df)\n",
    "    \n",
    "smoothed_df = pd.concat(smoothed_df_list)\n",
    "smoothed_df = smt.sort_and_reindex(smoothed_df)\n",
    "\n",
    "display(pd.concat((smoothed_df.head(), smoothed_df.tail())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add information from smoothed seismicity from areal sources\n",
    "columns_to_copy = ['zoneid', 'zmax', 'zmin', 'tectonic subregion',\n",
    "                   'avalue', 'bvalue', 'stdbvalue', 'mmax', 'stdmmax',\n",
    "                   'rake', 'dip', 'strike', 'aspect ratio', 'msr']\n",
    "\n",
    "distances = np.full((len(smoothed_df),len(areal_source_df)), np.inf)\n",
    "for i, area_series in areal_source_df.iterrows():\n",
    "\n",
    "    at_depth = (smoothed_df['layerid'] == area_series['layerid']).values\n",
    "    lons = smoothed_df.loc[at_depth, 'longitude'].values\n",
    "    lats = smoothed_df.loc[at_depth, 'latitude'].values\n",
    "    mesh = geo.mesh.Mesh(lons, lats)\n",
    "    poly_coords = area_series['polygon coordinates']\n",
    "    points = [geo.point.Point(lat, lon) for lat, lon in poly_coords]\n",
    "    polygon = geo.polygon.Polygon(points)\n",
    "    distances[at_depth, i] = polygon.distances(mesh)\n",
    "\n",
    "index_min = distances.argmin(axis=1)\n",
    "for i, area_series in areal_source_df.iterrows():\n",
    "    picked = i == index_min\n",
    "    for column in columns_to_copy:\n",
    "        smoothed_df.loc[picked, column] = area_series[column]\n",
    "    above_min = area_series['mmin'] > smoothed_df['mmin']\n",
    "    smoothed_df.loc[picked & above_min, 'mmin'] = area_series['mmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((smoothed_df[~smoothed_df['zoneid'].isnull()].head(),\n",
    "           smoothed_df[~smoothed_df['zoneid'].isnull()].tail()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(smoothed_df[(smoothed_df['longitude'] == 98) & (smoothed_df['latitude'] == 3.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smoothed_source_data_file = 'smoothed_source_model'\n",
    "# write everything to TSV file\n",
    "smoothed_output_df = smt.add_binwise_rates(\n",
    "    smt.twin_source_by_magnitude(smoothed_df))\n",
    "smoothed_output_df.to_csv(smoothed_source_data_file + '.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write NRML source files with megathrust sources twinned\n",
    "for min_mag in min_mags:\n",
    "    indices = ((smoothed_df['mmin'] == min_mag) &\n",
    "               ~smoothed_df['tectonic subregion'].isnull()) \n",
    "    smoothed_source_list = smt.source_df_to_list(\n",
    "        smt.twin_source_by_magnitude(smoothed_df.loc[indices, :]))\n",
    "    smoothed_source_model = src.source_model.mtkSourceModel(\n",
    "        identifier='1', \n",
    "        name='%s smoothed m_min=%g' % (model_path, min_mag), \n",
    "        sources=smoothed_source_list)\n",
    "    file_name = smoothed_source_data_file + '_mmin%g.xml' % min_mag \n",
    "    smoothed_source_model.serialise_to_nrml(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write each layer to KML with added binwise rates\n",
    "smoothed_kml_df = smt.add_binwise_rates(smoothed_df)\n",
    "smoothed_kml_df.drop(['zmax','zmin','aspect ratio'], axis=1, inplace=True)\n",
    "for min_mag in reversed(min_mags):\n",
    "    for layer_id in reversed(layer_ids):\n",
    "        indices = ((smoothed_df['mmin'] == min_mag) &\n",
    "                   (smoothed_df['layerid'] == layer_id))\n",
    "        temp_df = smoothed_kml_df.drop(['mmin','layerid'], axis=1)\n",
    "        smt.source_df_to_kml(temp_df.loc[indices, :], \n",
    "            '%s layer %d mmin %g' % (smoothed_source_data_file, layer_id, min_mag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PseudoCatalogue:\n",
    "    \"\"\"\n",
    "    ugly hack: construct pseudo-cataloge from average mechanisms\n",
    "    \"\"\"\n",
    "    def __init__(self, source_model, select_depth='all'):\n",
    "        data = []\n",
    "        oq_sources = source_model.convert_to_oqhazardlib(tom.PoissonTOM(1.0))\n",
    "        for source in oq_sources:\n",
    "            \n",
    "            longitude = np.mean(source.polygon.lons)\n",
    "            latitude = np.mean(source.polygon.lats)\n",
    "            strike = source.nodal_plane_distribution.data[0][1].strike\n",
    "            dip = source.nodal_plane_distribution.data[0][1].dip\n",
    "            rake = source.nodal_plane_distribution.data[0][1].rake\n",
    "            magnitude = source.get_min_max_mag()[1]*2.5\n",
    "            depth = source.hypocenter_distribution.data[0][1]\n",
    "            name = source.id\n",
    "            \n",
    "            if select_depth == 'all' or depth == select_depth:\n",
    "                data.append({'longitude': longitude, 'latitude': latitude,\n",
    "                             'strike1': strike, 'dip1': dip, 'rake1': rake,\n",
    "                             'magnitude': magnitude, 'depth': depth, 'id': name})\n",
    "        self.data = pd.DataFrame(data)\n",
    "\n",
    "    def get_number_tensors(self):\n",
    "        return len(self.data.magnitude)\n",
    "\n",
    "catalogue = PseudoCatalogue(areal_source_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_config = {\"min_lon\": 60, \"max_lon\": 105, \n",
    "              \"min_lat\": 0,  \"max_lat\": 40, \"resolution\": \"l\",\n",
    "              \"parallel_meridian_spacing\": 5}\n",
    "parser = nrmlSourceModelParser(areal_source_model_file + '.xml')\n",
    "\n",
    "for depth in sorted(list(set(catalogue.data['depth']))):\n",
    "    basemap = HMTKBaseMap(map_config, '')\n",
    "\n",
    "    source_model_read = parser.read_file('Areal Source Model')\n",
    "    selected_sources = [source for source in source_model_read.sources \n",
    "                    if source.hypo_depth_dist.data[0][1] == depth]\n",
    "    source_model_read.sources = selected_sources    \n",
    "    selected_catalogue = PseudoCatalogue(source_model_read)\n",
    "\n",
    "    basemap.add_source_model(source_model_read, overlay=True) \n",
    "    basemap.add_focal_mechanism(selected_catalogue, magnitude=False)\n",
    "    for _, item in selected_catalogue.data.iterrows():\n",
    "        plt.annotate(s=item.id, xy=(item.longitude, item.latitude))\n",
    "    plt.savefig(\"ArealModel%gkmDepth.png\" % depth, dpi=300,\n",
    "                transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# this fails because the source file is too large to parse\n",
    "map_config = {\"min_lon\": 60, \"max_lon\": 105, \n",
    "              \"min_lat\": 0,  \"max_lat\": 40, \"resolution\": \"l\",\n",
    "              \"parallel_meridian_spacing\": 5}\n",
    "parser = nrmlSourceModelParser(smoothed_source_data_file + '_mmin%g.xml' % 4.5)\n",
    "\n",
    "for depth in sorted(list(set(catalogue.data['depth']))):\n",
    "    basemap = HMTKBaseMap(map_config, '')\n",
    "\n",
    "    source_model_read = parser.read_file('Smoothed Source Model')\n",
    "    selected_sources = [source for source in source_model_read.sources \n",
    "                    if source.hypo_depth_dist.data[0][1] == depth]\n",
    "    source_model_read.sources = selected_sources    \n",
    "    selected_catalogue = PseudoCatalogue(source_model_read)\n",
    "\n",
    "    basemap.add_source_model(source_model_read, overlay=True) \n",
    "    basemap.add_focal_mechanism(selected_catalogue, magnitude=False)\n",
    "    for _, item in selected_catalogue.data.iterrows():\n",
    "        plt.annotate(s=item.id, xy=(item.longitude, item.latitude))\n",
    "    plt.savefig(\"ArealModel%gkmDepth.png\" % depth, dpi=300,\n",
    "                transparent=True, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(210 - 30)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
