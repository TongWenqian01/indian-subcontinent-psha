{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Setup and Post-Processing for Nath & Thingbaijam (2012)\n",
    "\n",
    "Mostly minor reformatting.\n",
    "\n",
    "## Job Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import codecs\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these imports have been obsolete since the introduction of --export csv\n",
    "# associated code needs re-writing to work\n",
    "import openquake.oq_output.hazard_curve_converter as hcc\n",
    "import openquake.oq_output.hazard_map_converter as hmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../utilities')\n",
    "import source_model_tools as smt  # noqa\n",
    "import toolbox as tb  # noqa\n",
    "import gmpe_tools as gmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nath_path = '../Data/nath2012probabilistic'\n",
    "sites_table = os.path.join(nath_path, 'Table 3.csv')\n",
    "df_table3 = pd.read_csv(sites_table, skiprows=1)\n",
    "df_table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sites = df_table3[['Longitude (°E)', 'Latitude (°N)']]\n",
    "sites_csv = 'NT2012_Table_3_lon_lat.csv'\n",
    "print('sites_csv = %s' % sites_csv)\n",
    "df_sites.to_csv(sites_csv, header=False, index=False, float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_imts = ['PGA','SA(0.2)','SA(1.0)']\n",
    "map_tables = ['India_pga.csv','India_psa_pt2sec.csv','India_psa_1sec.csv']\n",
    "map_table_list = [os.path.join(nath_path, item) for item in map_tables]\n",
    "map_table_df_list = [pd.read_csv(file_name) for file_name in map_table_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = map_table_df_list[0][['lon', 'lat']]\n",
    "map_csv = 'NT2012_Figure_7_Indian_subcontinent_lon_lat.csv'\n",
    "print('sites_csv = %s' % map_csv)\n",
    "df_map.to_csv(map_csv, header=False, index=False, float_format='%g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_spacing = 2.\n",
    "himalaya_csv = 'NT2012_Figure_5_Himalayas_lon_lat.csv'\n",
    "df_Himalayas = pd.read_csv(himalaya_csv, names=['lon','lat'])\n",
    "keep = (((df_Himalayas['lon'] % reduced_spacing) == 0) &\n",
    "        ((df_Himalayas['lat'] % reduced_spacing) == 0))\n",
    "reduced_csv = himalaya_csv.replace('.csv', '_reduced.csv')\n",
    "df_Himalayas[keep].to_csv(reduced_csv, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# areal_only_no_fmd_uncertainty_map\n",
    "n_gmpe_lt_rlzs = 3*3*2*2*3*2*3*3*2\n",
    "print('n_gmpe_lt_rlzs = %d' % n_gmpe_lt_rlzs)\n",
    "n_src_lt_rlzs = 1\n",
    "print('n_src_lt_rlzs = %d' % n_src_lt_rlzs)\n",
    "n_lt_rlzs = n_gmpe_lt_rlzs*n_src_lt_rlzs\n",
    "print('n_lt_rlzs = %d' % n_lt_rlzs)\n",
    "n_sites = len(df_map)\n",
    "print('n_sites = %d' % n_sites)\n",
    "n_imt_iml = len(map_imts)\n",
    "print('n_imt_iml = %d' % n_imt_iml)\n",
    "n_curves = n_lt_rlzs*n_sites*n_imt_iml\n",
    "print('n_curves = n_lt_rlzs*n_sites*n_imt_iml = %d*%d*%d = %d' %\n",
    "     (n_lt_rlzs, n_sites, n_imt_iml, n_curves))\n",
    "n_files = n_lt_rlzs*n_imt_iml\n",
    "print('n_files = n_lt_rlzs*n_imt_iml = %d*%d = %d' %\n",
    "     (n_lt_rlzs, n_imt_iml, n_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALC_ID = 123\n",
    "export_dir = '../HazardOutputs'\n",
    "export_path = os.path.join(os.path.abspath(export_dir), 'calc_%d' % CALC_ID)\n",
    "\n",
    "exported_files = [os.path.join(dir_path, f)\n",
    "    for dir_path, dirnames, files in os.walk(export_path)\n",
    "    for f in files if f.endswith('.xml')]\n",
    "exported_files\n",
    "print(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_keys = ['hazard_curve-', 'mean']\n",
    "output_subset = [item for item in exported_files if all(key in item for key in subset_keys)]\n",
    "output_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cities = np.array([['Bangalore', 'Bhuj', 'Chennai'],\n",
    "                        ['Guwahati', 'Jabalpur', 'Kolkata'],\n",
    "                        ['Koyna', 'Mumbai', 'New Delhi']])\n",
    "a_maxima = np.array([[1, 2.5, 1], [2, 1.2, 1.2], [2, 1.2, 1.2]])\n",
    "\n",
    "fig, axes = plt.subplots(plot_cities.shape[0], plot_cities.shape[1], \n",
    "                         figsize=(12,8))\n",
    "\n",
    "for file_name in output_subset:\n",
    "    hcm = hcc.read_hazard_curves(file_name)\n",
    "    label = os.path.split(file_name)[1]\n",
    "    \n",
    "    if hcm['investigation_time'] != T_inv:\n",
    "        warnings.warn(\n",
    "            'Investigation time %g instead of expected %g' %\n",
    "            hcm['investigation_time'], T_inv, UserWarning)\n",
    "    \n",
    "    if hcm['imt'] == 'PGV':\n",
    "        warnings.warn('IMT %s units not properly reflected in axis label.' % \n",
    "                      hcm['imt'], UserWarning)\n",
    "        \n",
    "    for row in hcm['curves']:\n",
    "        lon, lat = row[:2]\n",
    "        in_table3 = ((df_table3['Longitude (°E)'] == lon) & \n",
    "                     (df_table3['Latitude (°N)'] == lat))\n",
    "        \n",
    "        if any(in_table3):\n",
    "            city = df_table3.loc[in_table3, 'City'].values[0]\n",
    "        else:\n",
    "            print('Unknown site %g°N %g°E. Ignoring ...' % (lat, lon))\n",
    "            continue\n",
    "            \n",
    "        if city in plot_cities:\n",
    "            ax = axes[plot_cities == city][0]\n",
    "            poes = row[2:]\n",
    "            ax.semilogy(hcm[\"imls\"], poes, label=hcm['imt'])\n",
    "\n",
    "for ax, city, a_max in zip(axes.ravel(), plot_cities.ravel(), a_maxima.ravel()):\n",
    "    ax.set_xlim((0, a_max))\n",
    "    ax.set_ylim((1e-4, ax.get_ylim()[1]))\n",
    "    for prob, label, linestyle in zip(poes_inv, poe_labels, ['--', '-.', ':']):\n",
    "        ax.axhline(prob, label=label, color='0.5', linestyle=linestyle)\n",
    "    anchored_text = AnchoredText(city, loc=tb.LOC_CODE['upper center'], frameon=False)\n",
    "    ax.add_artist(anchored_text)\n",
    "\n",
    "axes[1,0].set_ylabel('Annual probability of exceedence')\n",
    "axes[-1,1].set_xlabel('Ground Acceleration [g]')\n",
    "axes[1,-1].legend(loc='center left', frameon=False, bbox_to_anchor=(1, 0.5))\n",
    "fig.savefig('Figure_6_Calc_%d.pdf' % CALC_ID, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_keys = ['hazard_map', 'mean']\n",
    "output_subset = [item for item in exported_files if all(key in item for key in subset_keys)]\n",
    "output_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_csv_list = [hmc.save_hazard_map_to_csv(file_name, force_overwrite=True) for file_name in output_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pga = pd.read_csv(map_csv_list[0], header=1)\n",
    "\n",
    "df_compare = pd.merge(df_table3, df_pga, left_on = 'Longitude (°E)', right_on = 'lon')\n",
    "drop_cols = ['BIS','zone','GSHAP','Other','Reference','lon','lat']\n",
    "df_compare.drop(drop_cols, axis=1, inplace=True)\n",
    "df_compare.rename(columns={'Present':'NT2012', 'iml': 'A2016'}, inplace=True)\n",
    "df_compare['Error (%)'] = (100*(df_compare['A2016']/df_compare['NT2012'] - 1)).round(1)\n",
    "df_compare.sort(columns='NT2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare['Error (%)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('Table_3_Calc_%d.tex' % CALC_ID, 'w', 'utf-8') as file_object:\n",
    "    df_compare.to_latex(file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_compare.to_csv('Table_3_Calc_%d.csv' % CALC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.log10(3**223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oq",
   "language": "python",
   "name": "oq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
